<!DOCTYPE html>
<html lang="en" class="light">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multimodal Efficiency: A Survey | Aneek Roy</title>
    <!-- Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    fontFamily: {
                        sans: ['Inter', 'system-ui', 'sans-serif'],
                        serif: ['Merriweather', 'Georgia', 'serif'],
                    },
                    colors: {
                        academic: {
                            50: '#f0f9ff',
                            100: '#e0f2fe',
                            200: '#bae6fd',
                            300: '#7dd3fc',
                            400: '#38bdf8',
                            500: '#0ea5e9',
                            600: '#0284c7', // Primary Brand Color
                            700: '#0369a1',
                            800: '#075985',
                            900: '#0c4a6e',
                            950: '#082f49', // Dark Mode BG
                        }
                    }
                }
            }
        }
    </script>
    <!-- Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <!-- Font Awesome -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Merriweather:ital,wght@0,300;0,400;0,700;1,300&display=swap" rel="stylesheet">

    <!-- Application Structure Plan:
         1. Navbar: Updated Logo (Transformer Block Diagram).
         2. Header: Academic Title, Author (Aneek Roy).
         3. Content: Stats, Chart, Literature Review.
         4. Styling: Formal academic blue theme.
         UPDATED: Now includes 38 papers (original 20 + 18 new)
    -->
    <!-- CONFIRMATION: NO SVG files used (Inline SVG only). NO Mermaid JS used. -->

    <style>
        body { font-family: 'Merriweather', serif; }
        h1, h2, h3, h4, h5, h6, nav, button, input, select, .sans-font { font-family: 'Inter', sans-serif; }
        html { scroll-behavior: smooth; }
        .chart-container { position: relative; width: 100%; height: 400px; }
        .dark body { color: #cbd5e1; }
        .dark h1, .dark h2, .dark h3 { color: #f1f5f9; }
        ::-webkit-scrollbar { width: 8px; }
        ::-webkit-scrollbar-track { background: transparent; }
        ::-webkit-scrollbar-thumb { background: #0284c7; border-radius: 4px; }
        /* View transitions */
        .view { display: none; }
        .view.active { display: block; }
        /* Methodology cards hover */
        .methodology-card { cursor: pointer; transition: all 0.2s; }
        .methodology-card:hover { transform: translateY(-2px); box-shadow: 0 4px 12px rgba(2, 132, 199, 0.15); }
    </style>
</head>
<body class="bg-white text-slate-800 dark:bg-academic-950 dark:text-slate-300 transition-colors duration-300 antialiased leading-relaxed">

    <!-- Navigation -->
    <nav class="sticky top-0 z-50 bg-white/95 dark:bg-academic-950/95 backdrop-blur-sm border-b border-slate-200 dark:border-slate-800 transition-colors">
        <div class="max-w-5xl mx-auto px-6 h-16 flex justify-between items-center">
            <!-- Transformer Architecture Logo (SVG) -->
            <a href="blogpost.html" class="flex items-center gap-4 cursor-pointer group">
                <svg viewBox="0 0 100 100" class="w-10 h-10 text-academic-600 dark:text-academic-400 stroke-current group-hover:opacity-80 transition-opacity" fill="none" stroke-width="6" stroke-linecap="round" stroke-linejoin="round">
                    <!-- Internal Layers (Attention & FeedForward) -->
                    <rect x="35" y="60" width="40" height="15" rx="3" stroke-width="5" />
                    <rect x="35" y="30" width="40" height="15" rx="3" stroke-width="5" />

                    <!-- Vertical Data Flow -->
                    <path d="M 55 90 L 55 75" /> <!-- Input -->
                    <path d="M 55 60 L 55 45" /> <!-- Inter-layer -->
                    <path d="M 55 30 L 55 15" /> <!-- Output -->

                    <!-- Residual Skip Connection (The Signature) -->
                    <path d="M 35 70 C 10 70, 10 35, 35 35" stroke-width="4" stroke-dasharray="0" opacity="0.8"/>

                    <!-- Attention Heads (Dots inside first block) -->
                    <circle cx="45" cy="67.5" r="1.5" fill="currentColor" stroke="none" />
                    <circle cx="55" cy="67.5" r="1.5" fill="currentColor" stroke="none" />
                    <circle cx="65" cy="67.5" r="1.5" fill="currentColor" stroke="none" />
                </svg>
                <div class="flex flex-col">
                    <span class="font-bold text-lg tracking-tight text-slate-900 dark:text-white leading-none">Aneek Roy</span>
                    <span class="text-[10px] uppercase tracking-widest text-academic-600 dark:text-academic-400 font-medium">Research Blog</span>
                </div>
            </a>

            <div class="flex items-center gap-6">
                <a href="index.html" class="hidden md:block text-sm font-medium text-slate-500 hover:text-academic-600 dark:text-slate-400 dark:hover:text-academic-400 transition-colors sans-font">← Home</a>
                <a href="#literature" class="hidden md:block text-sm font-medium text-slate-500 hover:text-academic-600 dark:text-slate-400 dark:hover:text-academic-400 transition-colors sans-font">Literature</a>
                <a href="#analysis" class="hidden md:block text-sm font-medium text-slate-500 hover:text-academic-600 dark:text-slate-400 dark:hover:text-academic-400 transition-colors sans-font">Analysis</a>

                <!-- Dark Mode Toggle -->
                <button onclick="toggleTheme()" class="w-9 h-9 rounded-lg bg-slate-100 dark:bg-slate-800 flex items-center justify-center text-slate-600 dark:text-slate-400 hover:bg-academic-100 dark:hover:bg-academic-900 transition-all">
                    <i class="fas fa-moon dark:hidden text-sm"></i>
                    <i class="fas fa-sun hidden dark:block text-sm"></i>
                </button>
            </div>
        </div>
    </nav>

    <!-- Main Article Container -->
    <main class="max-w-4xl mx-auto px-6 py-16">

    <!-- View: Main Blog -->
    <div id="view-main" class="view active">

        <!-- Article Header -->
        <header class="mb-12 border-b border-slate-200 dark:border-slate-800 pb-12">
            <span class="text-academic-600 dark:text-academic-400 font-bold text-sm uppercase tracking-wider mb-3 block sans-font">Survey Blog • Jan 2026</span>
            <h1 class="text-4xl md:text-5xl font-bold text-slate-900 dark:text-white mb-6 leading-tight tracking-tight">
                Efficient Multimodal Encoders: <br>A Comparative Review (2020-2025)
            </h1>
            <div class="flex items-center gap-4 text-sm sans-font text-slate-500 dark:text-slate-400">
                <div class="flex items-center gap-2">
                    <div class="w-8 h-8 rounded-full bg-academic-100 dark:bg-academic-900 flex items-center justify-center text-academic-700 dark:text-academic-300 font-bold text-xs border border-academic-200 dark:border-academic-800">AR</div>
                    <span>Aneek Roy</span>
                </div>
                <span>•</span>
                <span>20 min read</span>
                <span>•</span>
                <span class="bg-slate-100 dark:bg-slate-800 px-2 py-0.5 rounded text-xs font-medium">Updated: Jan 2026</span>
            </div>
        </header>

        <!-- Abstract -->
        <div class="bg-academic-50 dark:bg-academic-900/20 p-6 rounded-lg border-l-4 border-academic-500 mb-12">
            <h3 class="font-bold text-sm uppercase text-academic-700 dark:text-academic-400 mb-2 sans-font">Abstract</h3>
            <p class="text-slate-700 dark:text-slate-300 italic text-base">
                This post surveys the rapid evolution of efficient encoders for multimodal (Vision-Language) models. We analyze key contributions from ICML, ICLR, NeurIPS, CVPR, and ECCV over the past 5 years, highlighting the transition from large-scale frozen transformers to dynamic, token-pruned architectures. We discuss methodologies including "Frozen" LLM adaptation, Q-Former bridges, Reinforcement Learning-based token selection, hybrid CNN-Transformer architectures, and attention-based token pruning.
            </p>
        </div>

        <!-- Section 1: Introduction & Stats -->
        <article class="prose dark:prose-invert max-w-none mb-16">
            <p class="text-lg mb-6">
                The scaling laws of deep learning have historically driven performance improvements through massive parameter increases. However, the deployment of multimodal models on edge devices requires a fundamental shift towards efficiency. Since the introduction of CLIP in 2021, the research community has achieved approximately a <strong>54x reduction in parameter count</strong> for comparable zero-shot accuracy on specific benchmarks.
            </p>
            <p class="mb-8">
                The following figure summarizes the aggregate progress observed across major conferences.
            </p>

            <!-- Figure 1: Stats Grid -->
            <div class="grid grid-cols-2 md:grid-cols-4 gap-4 not-prose mb-8 sans-font">
                <div class="p-4 bg-white dark:bg-slate-800 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                    <div class="text-3xl font-bold text-academic-600 dark:text-academic-400">85x</div>
                    <div class="text-xs text-slate-500 uppercase font-medium mt-1">TTFT Speedup</div>
                </div>
                <div class="p-4 bg-white dark:bg-slate-800 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                    <div class="text-3xl font-bold text-academic-600 dark:text-academic-400">~0.8G</div>
                    <div class="text-xs text-slate-500 uppercase font-medium mt-1">Min FLOPs (2025)</div>
                </div>
                <div class="p-4 bg-white dark:bg-slate-800 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                    <div class="text-3xl font-bold text-academic-600 dark:text-academic-400">RL+MoE</div>
                    <div class="text-xs text-slate-500 uppercase font-medium mt-1">Emerging Trends</div>
                </div>
                <div class="p-4 bg-white dark:bg-slate-800 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                    <div class="text-3xl font-bold text-academic-600 dark:text-academic-400">61</div>
                    <div class="text-xs text-slate-500 uppercase font-medium mt-1">Papers Reviewed</div>
                </div>
            </div>
            <figcaption class="text-center text-sm text-slate-500 dark:text-slate-400 sans-font italic mb-12">
                <strong>Figure 1.</strong> Key efficiency metrics derived from the surveyed literature (2020-2025).
            </figcaption>
        </article>

        <!-- Section 2: Analysis Chart -->
        <section id="analysis" class="mb-20 scroll-mt-24">
            <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4 sans-font">2. The Efficiency Landscape</h2>
            <p class="mb-6 text-slate-700 dark:text-slate-300">
                To visualize the trade-off between computational cost and model performance, we map key papers onto a FLOPs vs. Accuracy plane. The ideal trajectory moves towards the top-left corner (High Accuracy, Low Compute).
            </p>

            <!-- Chart Control -->
            <div class="flex justify-end mb-4 sans-font">
                <div class="inline-flex rounded-md shadow-sm" role="group">
                    <button type="button" onclick="updateChartMode('flops')" id="btn-flops" class="px-4 py-2 text-xs font-medium text-white bg-academic-600 border border-academic-600 rounded-l-lg hover:bg-academic-700 transition-all">
                        FLOPs
                    </button>
                    <button type="button" onclick="updateChartMode('params')" id="btn-params" class="px-4 py-2 text-xs font-medium text-slate-700 bg-white border border-slate-200 rounded-r-lg hover:bg-slate-50 dark:bg-slate-800 dark:border-slate-700 dark:text-white dark:hover:bg-slate-700 transition-all">
                        Parameters
                    </button>
                </div>
            </div>

            <!-- Figure 2: Interactive Chart -->
            <div class="bg-white dark:bg-slate-900 p-4 rounded-xl border border-slate-200 dark:border-slate-800 shadow-sm">
                <div class="chart-container">
                    <canvas id="frontierChart"></canvas>
                </div>
            </div>
            <figcaption class="text-center text-sm text-slate-500 dark:text-slate-400 sans-font italic mt-4">
                <strong>Figure 2.</strong> Pareto frontier of efficient encoders. Blue: NeurIPS, Cyan: ICML, Light Blue: ICLR, Green: CVPR, Orange: ECCV.
                <br><span class="text-xs opacity-75">Note: Newer papers (2025) are represented by larger nodes.</span>
            </figcaption>
        </section>

        <!-- Section 3: Methodology Breakdown -->
        <section class="mb-20">
            <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-6 sans-font">3. Methodological Categorization</h2>
            <p class="text-slate-600 dark:text-slate-400 mb-6 text-sm">Click on a category to explore papers using that methodology.</p>
            <div class="grid md:grid-cols-2 gap-8 sans-font">
                <div onclick="navigateTo('methodology/frozen-foundations')" class="methodology-card bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-100 dark:border-slate-800 hover:border-academic-400 dark:hover:border-academic-600">
                    <div class="flex items-center gap-3 mb-2">
                        <i class="fas fa-snowflake text-academic-500"></i>
                        <h3 class="font-bold text-lg text-academic-700 dark:text-academic-400">Frozen Foundations</h3>
                    </div>
                    <p class="text-sm text-slate-600 dark:text-slate-400 leading-relaxed">
                        Methods like <strong>BLIP-2</strong>, <strong>Frozen</strong>, and <strong>Flamingo</strong> leverage pre-trained LLMs without updating their weights. They introduce lightweight bridging modules (e.g., Q-Former, Perceiver Resampler) to align visual features, drastically reducing training costs.
                    </p>
                    <div class="mt-3 flex items-center gap-2 text-xs text-academic-600 dark:text-academic-400">
                        <span>View papers</span>
                        <i class="fas fa-arrow-right"></i>
                    </div>
                </div>
                <div onclick="navigateTo('methodology/dynamic-computation')" class="methodology-card bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-100 dark:border-slate-800 hover:border-academic-400 dark:hover:border-academic-600">
                    <div class="flex items-center gap-3 mb-2">
                        <i class="fas fa-bolt text-academic-500"></i>
                        <h3 class="font-bold text-lg text-academic-700 dark:text-academic-400">Dynamic Computation</h3>
                    </div>
                    <p class="text-sm text-slate-600 dark:text-slate-400 leading-relaxed">
                        Recent works like <strong>VisionThink (2025)</strong> and <strong>FastV (2024)</strong> utilize RL or attention-based methods to dynamically adjust the visual token budget based on complexity, departing from static architectural pruning.
                    </p>
                    <div class="mt-3 flex items-center gap-2 text-xs text-academic-600 dark:text-academic-400">
                        <span>View papers</span>
                        <i class="fas fa-arrow-right"></i>
                    </div>
                </div>
                <div onclick="navigateTo('methodology/hybrid-architectures')" class="methodology-card bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-100 dark:border-slate-800 hover:border-academic-400 dark:hover:border-academic-600">
                    <div class="flex items-center gap-3 mb-2">
                        <i class="fas fa-layer-group text-academic-500"></i>
                        <h3 class="font-bold text-lg text-academic-700 dark:text-academic-400">Hybrid Architectures</h3>
                    </div>
                    <p class="text-sm text-slate-600 dark:text-slate-400 leading-relaxed">
                        <strong>MobileViT</strong>, <strong>FastVLM</strong>, and <strong>TinyViT</strong> combine the local feature extraction of CNNs with global attention of Transformers, achieving mobile-friendly efficiency without sacrificing accuracy.
                    </p>
                    <div class="mt-3 flex items-center gap-2 text-xs text-academic-600 dark:text-academic-400">
                        <span>View papers</span>
                        <i class="fas fa-arrow-right"></i>
                    </div>
                </div>
                <div onclick="navigateTo('methodology/token-compression')" class="methodology-card bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-100 dark:border-slate-800 hover:border-academic-400 dark:hover:border-academic-600">
                    <div class="flex items-center gap-3 mb-2">
                        <i class="fas fa-compress-alt text-academic-500"></i>
                        <h3 class="font-bold text-lg text-academic-700 dark:text-academic-400">Token Compression</h3>
                    </div>
                    <p class="text-sm text-slate-600 dark:text-slate-400 leading-relaxed">
                        <strong>ToMe</strong>, <strong>SparseVLM</strong>, and <strong>LLaVA-Mini</strong> reduce visual tokens via merging, pruning, or compression. These methods achieve 50-77% FLOPs reduction while maintaining task performance.
                    </p>
                    <div class="mt-3 flex items-center gap-2 text-xs text-academic-600 dark:text-academic-400">
                        <span>View papers</span>
                        <i class="fas fa-arrow-right"></i>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 4: Literature Review (Grid) -->
        <section id="literature" class="mb-20 scroll-mt-24">
            <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-6 sans-font flex justify-between items-center">
                <span>4. Literature Review</span>
                <span class="text-xs font-normal text-slate-500 bg-slate-100 dark:bg-slate-800 px-2 py-1 rounded">61 Papers • Searchable</span>
            </h2>

            <!-- Search Controls -->
            <div class="flex flex-col md:flex-row gap-4 mb-8 sans-font">
                <div class="relative flex-grow">
                    <i class="fas fa-search absolute left-3 top-3 text-slate-400 text-sm"></i>
                    <input type="text" id="search-input" placeholder="Search by title, author, or keyword..."
                        class="w-full pl-9 pr-4 py-2.5 bg-slate-50 dark:bg-slate-900 border border-slate-200 dark:border-slate-700 rounded-md text-sm focus:ring-2 focus:ring-academic-500 focus:border-academic-500 outline-none transition-colors dark:text-white"
                        oninput="filterPapers()">
                </div>
                <div class="flex gap-2">
                    <select id="venue-filter" onchange="filterPapers()" class="px-3 py-2 bg-white dark:bg-slate-900 border border-slate-200 dark:border-slate-700 rounded-md text-sm text-slate-600 dark:text-slate-300 outline-none focus:border-academic-500 cursor-pointer">
                        <option value="all">All Venues</option>
                        <option value="NeurIPS">NeurIPS</option>
                        <option value="ICLR">ICLR</option>
                        <option value="ICML">ICML</option>
                        <option value="CVPR">CVPR</option>
                        <option value="ECCV">ECCV</option>
                        <option value="ICCV">ICCV</option>
                        <option value="AAAI">AAAI</option>
                        <option value="arXiv">arXiv</option>
                        <option value="ICASSP">ICASSP</option>
                        <option value="ISCA">ISCA</option>
                        <option value="Nature">Nature</option>
                        <option value="ACL">ACL</option>
                        <option value="CVPR-W">CVPR-W</option>
                    </select>
                </div>
            </div>

            <!-- Paper Grid -->
            <div id="paper-grid" class="grid grid-cols-1 md:grid-cols-2 gap-6 sans-font">
                <!-- Injected via JS -->
            </div>
        </section>

        <!-- Conclusion -->
        <section class="mb-16 border-t border-slate-200 dark:border-slate-800 pt-12">
            <h2 class="text-2xl font-bold text-slate-900 dark:text-white mb-4 sans-font">5. Conclusion</h2>
            <p class="text-lg text-slate-700 dark:text-slate-300">
                The trend towards efficiency in multimodal learning has shifted from simple parameter reduction to intelligent computation. While 2021-2023 focused on architectural bottlenecks (Perceiver, MobileViT, TinyViT), 2024-2025 has introduced dynamic, data-centric approaches where the model decides <em>where</em> to look and <em>how much</em> compute to expend. Key innovations include attention-based token pruning (FastV), hybrid vision encoders (FastVLM), and RL-driven adaptive resolution (VisionThink).
            </p>
        </section>

        <!-- Footer -->
        <footer class="text-center text-sm text-slate-500 dark:text-slate-500 sans-font pt-12 pb-8">
            <p class="mb-2">&copy; Last Updated: 06/01/2026 Aneek Roy. All rights reserved.</p>
        </footer>

    </div><!-- End View: Main Blog -->

    <!-- View: Methodology Page -->
    <div id="view-methodology" class="view">
        <!-- Breadcrumb -->
        <nav class="mb-8 sans-font">
            <ol class="flex items-center gap-2 text-sm">
                <li><a href="#" onclick="navigateTo(''); return false;" class="text-academic-600 dark:text-academic-400 hover:underline">Home</a></li>
                <li class="text-slate-400">/</li>
                <li id="methodology-breadcrumb" class="text-slate-600 dark:text-slate-400">Methodology</li>
            </ol>
        </nav>

        <!-- Header -->
        <header class="mb-10">
            <div class="flex items-center gap-4 mb-4">
                <div id="methodology-icon" class="w-12 h-12 rounded-xl bg-academic-100 dark:bg-academic-900 flex items-center justify-center text-academic-600 dark:text-academic-400 text-xl">
                    <i class="fas fa-folder"></i>
                </div>
                <div>
                    <h1 id="methodology-title" class="text-3xl font-bold text-slate-900 dark:text-white">Methodology</h1>
                    <p id="methodology-subtitle" class="text-sm text-slate-500 dark:text-slate-400 mt-1">Papers using this approach</p>
                </div>
            </div>
            <p id="methodology-description" class="text-slate-600 dark:text-slate-400 leading-relaxed"></p>
        </header>

        <!-- Stats -->
        <div class="grid grid-cols-3 gap-4 mb-10 sans-font">
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="methodology-count" class="text-2xl font-bold text-academic-600 dark:text-academic-400">0</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">Papers</div>
            </div>
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="methodology-avg-acc" class="text-2xl font-bold text-academic-600 dark:text-academic-400">0%</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">Avg Accuracy</div>
            </div>
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="methodology-avg-flops" class="text-2xl font-bold text-academic-600 dark:text-academic-400">0G</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">Avg FLOPs</div>
            </div>
        </div>

        <!-- Paper Grid -->
        <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-6 sans-font">Papers in this category</h2>
        <div id="methodology-paper-grid" class="grid grid-cols-1 md:grid-cols-2 gap-6 sans-font">
            <!-- Injected via JS -->
        </div>

        <!-- Back button -->
        <div class="mt-12 text-center">
            <button onclick="navigateTo('')" class="inline-flex items-center gap-2 px-4 py-2 bg-slate-100 dark:bg-slate-800 text-slate-700 dark:text-slate-300 rounded-lg hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors sans-font text-sm">
                <i class="fas fa-arrow-left"></i>
                Back to Blog
            </button>
        </div>
    </div><!-- End View: Methodology -->

    <!-- View: Paper Detail Page -->
    <div id="view-paper" class="view">
        <!-- Breadcrumb -->
        <nav class="mb-8 sans-font">
            <ol class="flex items-center gap-2 text-sm">
                <li><a href="#" onclick="navigateTo(''); return false;" class="text-academic-600 dark:text-academic-400 hover:underline">Home</a></li>
                <li class="text-slate-400">/</li>
                <li><a href="#" id="paper-methodology-link" class="text-academic-600 dark:text-academic-400 hover:underline">Methodology</a></li>
                <li class="text-slate-400">/</li>
                <li id="paper-breadcrumb" class="text-slate-600 dark:text-slate-400 truncate max-w-xs">Paper</li>
            </ol>
        </nav>

        <!-- Paper Header -->
        <header class="mb-10">
            <div class="flex items-center gap-3 mb-4">
                <span id="paper-venue-badge" class="inline-flex items-center px-3 py-1 rounded-full text-xs font-bold uppercase tracking-wider bg-academic-100 text-academic-700 dark:bg-academic-900 dark:text-academic-400">
                    Venue 2024
                </span>
                <span id="paper-year-dot" class="w-3 h-3 rounded-full bg-academic-500 hidden"></span>
            </div>
            <h1 id="paper-title" class="text-3xl font-bold text-slate-900 dark:text-white mb-3 leading-tight">Paper Title</h1>
            <p id="paper-authors" class="text-slate-500 dark:text-slate-400 italic">Authors</p>
        </header>

        <!-- Metrics -->
        <div class="grid grid-cols-3 gap-4 mb-10 sans-font">
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="paper-accuracy" class="text-2xl font-bold text-academic-600 dark:text-academic-400">--</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">Accuracy</div>
            </div>
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="paper-flops" class="text-2xl font-bold text-academic-600 dark:text-academic-400">--</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">FLOPs</div>
            </div>
            <div class="p-4 bg-white dark:bg-slate-900 rounded-lg border border-slate-200 dark:border-slate-700 text-center">
                <div id="paper-params" class="text-2xl font-bold text-academic-600 dark:text-academic-400">--</div>
                <div class="text-xs text-slate-500 uppercase font-medium mt-1">Parameters</div>
            </div>
        </div>

        <!-- Summary Section -->
        <section class="mb-10">
            <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4 sans-font flex items-center gap-2">
                <i class="fas fa-file-alt text-academic-500"></i>
                Summary
            </h2>
            <div class="bg-academic-50 dark:bg-academic-900/20 p-6 rounded-lg border-l-4 border-academic-500">
                <p id="paper-summary" class="text-slate-700 dark:text-slate-300 leading-relaxed">Summary text...</p>
            </div>
        </section>

        <!-- Methodology Section -->
        <section class="mb-10">
            <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4 sans-font flex items-center gap-2">
                <i class="fas fa-cogs text-academic-500"></i>
                Methodology
            </h2>
            <div class="bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-200 dark:border-slate-700">
                <p id="paper-methodology-detail" class="text-slate-600 dark:text-slate-400 leading-relaxed">Methodology details...</p>
            </div>
        </section>

        <!-- Future Directions Section -->
        <section class="mb-10">
            <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4 sans-font flex items-center gap-2">
                <i class="fas fa-lightbulb text-academic-500"></i>
                Future Directions
            </h2>
            <div class="bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-200 dark:border-slate-700">
                <p id="paper-future-directions" class="text-slate-600 dark:text-slate-400 leading-relaxed">Future research directions...</p>
            </div>
        </section>

        <!-- Tags -->
        <section class="mb-10">
            <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4 sans-font flex items-center gap-2">
                <i class="fas fa-tags text-academic-500"></i>
                Keywords
            </h2>
            <div id="paper-tags" class="flex flex-wrap gap-2 sans-font">
                <!-- Tags injected via JS -->
            </div>
        </section>

        <!-- Markdown Content Section -->
        <section class="mb-10">
            <h2 class="text-xl font-bold text-slate-900 dark:text-white mb-4 sans-font flex items-center gap-2">
                <i class="fas fa-file-alt text-academic-500"></i>
                Full Paper Details
            </h2>
            <div id="paper-markdown-section" class="bg-slate-50 dark:bg-slate-900 p-6 rounded-lg border border-slate-200 dark:border-slate-700">
                <div id="paper-markdown-loading" class="text-center py-8">
                    <i class="fas fa-spinner fa-spin text-2xl text-academic-600 dark:text-academic-400 mb-4"></i>
                    <p class="text-slate-500 dark:text-slate-400">Loading paper details...</p>
                </div>
                <div id="paper-markdown-content" class="hidden prose dark:prose-invert max-w-none">
                    <!-- Markdown content will be rendered here -->
                </div>
                <div id="paper-markdown-error" class="hidden text-center py-8">
                    <i class="fas fa-exclamation-triangle text-2xl text-yellow-500 mb-4"></i>
                    <p class="text-slate-500 dark:text-slate-400">Unable to load paper details.</p>
                </div>
            </div>
        </section>

        <!-- Actions -->
        <div class="flex flex-wrap gap-4 mb-12 sans-font">
            <a id="paper-source-link" href="#" target="_blank" class="inline-flex items-center gap-2 px-5 py-2.5 bg-academic-600 text-white rounded-lg hover:bg-academic-700 transition-colors">
                <i class="fas fa-external-link-alt"></i>
                View Paper
            </a>
            <button onclick="history.back()" class="inline-flex items-center gap-2 px-5 py-2.5 bg-slate-100 dark:bg-slate-800 text-slate-700 dark:text-slate-300 rounded-lg hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors">
                <i class="fas fa-arrow-left"></i>
                Back
            </button>
        </div>
    </div><!-- End View: Paper Detail -->

    </main>

    <!-- Modal for Paper Details -->
    <div id="paper-modal" class="fixed inset-0 z-[100] hidden" aria-labelledby="modal-title" role="dialog" aria-modal="true">
        <div class="fixed inset-0 bg-slate-900/60 backdrop-blur-sm transition-opacity" onclick="closeModal()"></div>
        <div class="fixed inset-0 z-10 w-screen overflow-y-auto">
            <div class="flex min-h-full items-center justify-center p-4 text-center sm:p-0">
                <div class="relative transform overflow-hidden rounded-lg bg-white dark:bg-academic-950 text-left shadow-xl transition-all sm:my-8 sm:w-full sm:max-w-xl border border-slate-200 dark:border-slate-800">
                    <div class="bg-white dark:bg-academic-950 px-4 pb-4 pt-5 sm:p-6 sm:pb-4">
                        <div class="sm:flex sm:items-start">
                            <div class="mt-3 text-center sm:ml-4 sm:mt-0 sm:text-left w-full sans-font">
                                <div class="flex justify-between items-center mb-4 border-b border-slate-100 dark:border-slate-800 pb-3">
                                    <span id="modal-venue" class="font-mono text-xs font-bold text-academic-600 dark:text-academic-400 uppercase">Venue</span>
                                    <button onclick="closeModal()" class="text-slate-400 hover:text-slate-600 dark:hover:text-slate-200"><i class="fas fa-times"></i></button>
                                </div>
                                <h3 class="text-xl font-bold leading-tight text-slate-900 dark:text-white mb-2" id="modal-title">Paper Title</h3>
                                <p class="text-sm text-slate-500 dark:text-slate-400 mb-6 italic" id="modal-authors">Authors</p>

                                <div class="bg-academic-50 dark:bg-slate-900 p-4 rounded text-sm text-slate-700 dark:text-slate-300 leading-relaxed mb-6 font-serif">
                                    <strong class="sans-font text-xs uppercase text-slate-400 block mb-1">Abstract Summary</strong>
                                    <span id="modal-summary">Summary text...</span>
                                </div>

                                <div class="grid grid-cols-3 gap-4 border-t border-slate-100 dark:border-slate-800 pt-4">
                                    <div>
                                        <div class="text-[10px] uppercase text-slate-400 font-bold">Accuracy</div>
                                        <div class="text-base font-bold text-slate-900 dark:text-white" id="modal-acc">--</div>
                                    </div>
                                    <div>
                                        <div class="text-[10px] uppercase text-slate-400 font-bold">Cost</div>
                                        <div class="text-base font-bold text-slate-900 dark:text-white" id="modal-flops">--</div>
                                    </div>
                                    <div>
                                        <div class="text-[10px] uppercase text-slate-400 font-bold">Size</div>
                                        <div class="text-base font-bold text-slate-900 dark:text-white" id="modal-params">--</div>
                                    </div>
                                </div>
                            </div>
                        </div>
                    </div>
                    <div class="bg-slate-50 dark:bg-slate-900 px-4 py-3 sm:flex sm:flex-row-reverse sm:px-6 gap-2 border-t border-slate-100 dark:border-slate-800">
                        <a id="modal-link" href="#" target="_blank" class="inline-flex w-full justify-center rounded-md bg-academic-600 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-academic-700 sm:w-auto transition-colors sans-font">
                            <i class="fas fa-external-link-alt mr-2"></i> View Source
                        </a>
                        <button type="button" id="modal-details-btn" class="mt-3 inline-flex w-full justify-center rounded-md bg-slate-700 dark:bg-slate-600 px-3 py-2 text-sm font-semibold text-white shadow-sm hover:bg-slate-800 dark:hover:bg-slate-500 sm:mt-0 sm:w-auto transition-colors sans-font">
                            <i class="fas fa-file-alt mr-2"></i> View Details
                        </button>
                        <button type="button" class="mt-3 inline-flex w-full justify-center rounded-md bg-white dark:bg-slate-800 px-3 py-2 text-sm font-semibold text-slate-900 dark:text-slate-300 shadow-sm ring-1 ring-inset ring-slate-300 dark:ring-slate-700 hover:bg-slate-50 dark:hover:bg-slate-700 sm:mt-0 sm:w-auto sans-font" onclick="closeModal()">Close</button>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- GitHub OAuth Modal -->
    <div id="github-oauth-modal" class="fixed inset-0 z-[100] hidden" role="dialog" aria-modal="true">
        <div class="fixed inset-0 bg-slate-900/60 backdrop-blur-sm transition-opacity" onclick="closeGitHubModal()"></div>
        <div class="fixed inset-0 z-10 w-screen overflow-y-auto">
            <div class="flex min-h-full items-center justify-center p-4 text-center sm:p-0">
                <div class="relative transform overflow-hidden rounded-lg bg-white dark:bg-academic-950 text-left shadow-xl transition-all sm:my-8 sm:w-full sm:max-w-lg border border-slate-200 dark:border-slate-800">
                    <div class="bg-white dark:bg-academic-950 px-6 py-6">
                        <div class="flex justify-between items-center mb-6">
                            <h3 class="text-xl font-bold text-slate-900 dark:text-white sans-font flex items-center gap-2">
                                <i class="fab fa-github"></i>
                                GitHub Authentication
                            </h3>
                            <button onclick="closeGitHubModal()" class="text-slate-400 hover:text-slate-600 dark:hover:text-slate-200">
                                <i class="fas fa-times"></i>
                            </button>
                        </div>

                        <!-- Step 1: Initial State -->
                        <div id="oauth-step-1" class="oauth-step">
                            <p class="text-slate-600 dark:text-slate-400 mb-4 text-sm leading-relaxed">
                                To upload annotated PDFs, you need to authenticate with GitHub using a Cloudflare Worker proxy.
                            </p>
                            <div class="bg-slate-50 dark:bg-slate-900 p-4 rounded-lg mb-4">
                                <p class="text-xs text-slate-500 mb-2 font-medium uppercase">Setup Instructions</p>
                                <ol class="text-xs text-slate-600 dark:text-slate-400 list-decimal list-inside space-y-1">
                                    <li>Deploy the worker from <code class="bg-slate-200 dark:bg-slate-700 px-1 rounded">workers/github-oauth-proxy.js</code></li>
                                    <li>Create a <a href="https://github.com/settings/developers" target="_blank" class="text-academic-600 dark:text-academic-400 hover:underline">GitHub OAuth App</a></li>
                                    <li>Add your Client Secret to Worker environment variables</li>
                                </ol>
                            </div>
                            <div class="mb-4">
                                <label class="block text-sm font-medium text-slate-700 dark:text-slate-300 mb-2 sans-font">Worker URL</label>
                                <input type="text" id="github-worker-url" placeholder="https://github-oauth-proxy.xxx.workers.dev"
                                    class="w-full px-4 py-2.5 bg-white dark:bg-slate-900 border border-slate-200 dark:border-slate-700 rounded-lg text-sm focus:ring-2 focus:ring-academic-500 focus:border-academic-500 outline-none transition-colors dark:text-white font-mono text-xs">
                            </div>
                            <div class="mb-6">
                                <label class="block text-sm font-medium text-slate-700 dark:text-slate-300 mb-2 sans-font">Client ID</label>
                                <input type="text" id="github-client-id" placeholder="Ov23li..."
                                    class="w-full px-4 py-2.5 bg-white dark:bg-slate-900 border border-slate-200 dark:border-slate-700 rounded-lg text-sm focus:ring-2 focus:ring-academic-500 focus:border-academic-500 outline-none transition-colors dark:text-white font-mono">
                            </div>
                            <button onclick="startGitHubAuth()" class="w-full py-2.5 bg-academic-600 text-white rounded-lg hover:bg-academic-700 transition-colors sans-font text-sm font-medium">
                                Start Authentication
                            </button>
                        </div>

                        <!-- Step 2: Device Code -->
                        <div id="oauth-step-2" class="oauth-step hidden">
                            <div class="text-center mb-6">
                                <div class="w-16 h-16 rounded-full bg-academic-100 dark:bg-academic-900 flex items-center justify-center mx-auto mb-4">
                                    <i class="fas fa-key text-2xl text-academic-600 dark:text-academic-400"></i>
                                </div>
                                <p class="text-slate-600 dark:text-slate-400 text-sm">
                                    Go to <a href="https://github.com/login/device" target="_blank" class="text-academic-600 dark:text-academic-400 font-medium hover:underline">github.com/login/device</a> and enter:
                                </p>
                            </div>
                            <div class="bg-slate-100 dark:bg-slate-800 p-6 rounded-lg text-center mb-6">
                                <p id="github-user-code" class="text-3xl font-mono font-bold text-slate-900 dark:text-white tracking-widest">XXXX-XXXX</p>
                            </div>
                            <div class="flex items-center justify-center gap-2 text-sm text-slate-500 mb-6">
                                <i class="fas fa-spinner fa-spin"></i>
                                <span>Waiting for authorization...</span>
                            </div>
                            <button onclick="cancelGitHubAuth()" class="w-full py-2.5 bg-slate-100 dark:bg-slate-800 text-slate-700 dark:text-slate-300 rounded-lg hover:bg-slate-200 dark:hover:bg-slate-700 transition-colors sans-font text-sm">
                                Cancel
                            </button>
                        </div>

                        <!-- Step 3: Success -->
                        <div id="oauth-step-3" class="oauth-step hidden">
                            <div class="text-center">
                                <div class="w-16 h-16 rounded-full bg-green-100 dark:bg-green-900 flex items-center justify-center mx-auto mb-4">
                                    <i class="fas fa-check text-2xl text-green-600 dark:text-green-400"></i>
                                </div>
                                <h4 class="text-lg font-bold text-slate-900 dark:text-white mb-2 sans-font">Authenticated!</h4>
                                <p class="text-slate-600 dark:text-slate-400 text-sm mb-6">You can now upload annotated PDFs.</p>
                                <button onclick="closeGitHubModal()" class="w-full py-2.5 bg-academic-600 text-white rounded-lg hover:bg-academic-700 transition-colors sans-font text-sm font-medium">
                                    Continue
                                </button>
                            </div>
                        </div>

                        <!-- Error State -->
                        <div id="oauth-error" class="hidden mt-4 p-4 bg-red-50 dark:bg-red-900/20 border border-red-200 dark:border-red-800 rounded-lg">
                            <p class="text-sm text-red-600 dark:text-red-400" id="oauth-error-text">An error occurred.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <!-- Toast Notification -->
    <div id="toast" class="fixed bottom-4 right-4 z-[200] hidden">
        <div class="bg-slate-900 dark:bg-slate-100 text-white dark:text-slate-900 px-4 py-3 rounded-lg shadow-lg flex items-center gap-3 sans-font text-sm">
            <i id="toast-icon" class="fas fa-check-circle text-green-400 dark:text-green-600"></i>
            <span id="toast-message">Success!</span>
        </div>
    </div>

    <!-- Scripts -->
    <script>
        // --- DATASET (UPDATED: 38 papers total) ---
        const papers = [
            // ========== ORIGINAL 20 PAPERS ==========
            {
                id: 1, title: "Multimodal Few-Shot Learning with Frozen Language Models", venue: "NeurIPS", year: 2021, authors: "Maria Tsimpoukelli et al.",
                summary: "Introduced the 'Frozen' approach: keeping the LLM frozen and only training a vision encoder to map images into the LLM's prompt space. A pioneer in parameter-efficient V+L learning.",
                accuracy: 55.0, flops: 3.5, params: 7000, tags: ["Frozen LLM"], link: "https://proceedings.neurips.cc/paper/2021/hash/01b7575c38dac42f3cfb7d500438b875-Abstract.html"
            },
            {
                id: 2, title: "Parameter Efficient Multimodal Transformers for Video", venue: "ICLR", year: 2021, authors: "Sangho Lee et al.",
                summary: "Proposed shared & low-rank Transformers to reduce parameters by ~97% for audio-visual modeling. Uses a novel sharing mechanism across modalities.",
                accuracy: 72.0, flops: 1.2, params: 4.5, tags: ["Low-Rank"], link: "https://arxiv.org/abs/2012.04124"
            },
            {
                id: 3, title: "Perceiver: General Perception with Iterative Attention", venue: "ICML", year: 2021, authors: "Andrew Jaegle et al.",
                summary: "A modality-agnostic architecture using a small set of latent units to attend to large inputs. Decouples compute cost from input size.",
                accuracy: 76.4, flops: 2.1, params: 42, tags: ["Bottleneck"], link: "http://proceedings.mlr.press/v139/jaegle21a.html"
            },
            {
                id: 4, title: "Align Before Fuse (ALBEF)", venue: "NeurIPS", year: 2021, authors: "Junnan Li et al.",
                summary: "Unified separate encoders and cross-modal encoder without object detectors. Uses momentum distillation for strong performance.",
                accuracy: 78.0, flops: 4.0, params: 210, tags: ["Distillation"], link: "https://arxiv.org/abs/2107.07651"
            },
            {
                id: 5, title: "Perceiver IO: Structured Inputs & Outputs", venue: "ICLR", year: 2022, authors: "Andrew Jaegle et al.",
                summary: "Extended Perceiver with flexible querying, enabling scalable multimodal encoding and decoding while maintaining linear complexity.",
                accuracy: 79.0, flops: 2.5, params: 45, tags: ["Linear Complexity"], link: "https://openreview.net/forum?id=fILj7WpI-g"
            },
            {
                id: 6, title: "BLIP-2: Bootstrapping Language-Image Pre-training", venue: "ICML", year: 2023, authors: "Junnan Li et al.",
                summary: "Uses a Q-Former to bridge frozen image encoders and frozen LLMs. Drastically cuts trainable parameters (54x fewer vs Flamingo).",
                accuracy: 84.5, flops: 3.0, params: 188, tags: ["Q-Former"], link: "https://proceedings.mlr.press/v202/li23q.html"
            },
            {
                id: 7, title: "Tuning LayerNorm in Attention", venue: "ICLR", year: 2024, authors: "Bingchen Zhao et al.",
                summary: "Showed that only tuning LayerNorm parameters in attention blocks yields robust adaptation, outperforming LoRA with 42% fewer trainable params.",
                accuracy: 82.0, flops: 2.8, params: 0.5, tags: ["Fine-Tuning"], link: "https://arxiv.org/abs/2312.11420"
            },
            {
                id: 8, title: "Glance2Gaze: Efficient Vision-Language Models", venue: "NeurIPS", year: 2025, authors: "Juan Chen et al.",
                summary: "Two-stage visual token reduction: global 'glance' fusion + selective 'gaze' token compression. Improved performance with equal/lower compute.",
                accuracy: 86.2, flops: 1.8, params: 150, tags: ["Token Pruning", "SOTA"], link: "https://openreview.net/pdf?id=gm65gK3uOJ"
            },
            {
                id: 9, title: "VisionThink: Smart VLM via RL", venue: "NeurIPS", year: 2025, authors: "Senqiao Yang et al.",
                summary: "RL-based dynamic visual token strategy. Uses low-res images by default and requests high-res only when needed by the prompt.",
                accuracy: 87.0, flops: 1.5, params: 300, tags: ["RL", "Dynamic"], link: "https://arxiv.org/abs/2507.13348"
            },
            {
                id: 10, title: "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning", venue: "NeurIPS", year: 2021, authors: "Paul Pu Liang et al.",
                summary: "Large-scale benchmark for multimodal learning with evaluation of time/space complexity. Reports inference time and memory for compact models suitable for mobile devices.",
                accuracy: 74.0, flops: 2.0, params: 50, tags: ["Benchmark"], link: "https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/37693cfc748049e45d87b8c7d8b9aacd-Paper-round1.pdf"
            },
            {
                id: 11, title: "OpenVision: Cost-Effective Vision Encoders for Multimodal Learning", venue: "CVPR", year: 2025, authors: "Xianhang Li et al.",
                summary: "Fully open, cost-effective vision encoders (5.9M-632.1M params) enabling efficient multimodal models for edge devices. Matches or surpasses OpenAI CLIP.",
                accuracy: 85.5, flops: 1.6, params: 5.9, tags: ["Open-Source", "Edge"], link: "https://openaccess.thecvf.com"
            },
            {
                id: 12, title: "LoCoMT: Low-Computational-Cost Multimodal Transformer", venue: "ICASSP", year: 2024, authors: "Sungeun Park, Edward Choi",
                summary: "Novel multimodal attention mechanism to reduce computational cost. Mitigates quadratic complexity as modalities increase. Reduces GFLOPs while matching performance.",
                accuracy: 81.0, flops: 1.4, params: 85, tags: ["Low-Cost", "Attention"], link: "https://ieeexplore.ieee.org"
            },
            {
                id: 13, title: "Dynamic Multimodal Fusion (DynMM)", venue: "CVPR", year: 2023, authors: "Zihui Xue, Radu Marculescu",
                summary: "Adaptive approach generating data-dependent forward paths. Gating function makes on-the-fly modality decisions. Reduces computation by 46.5% for sentiment analysis.",
                accuracy: 83.0, flops: 2.2, params: 120, tags: ["Dynamic", "Gating"], link: "https://openaccess.thecvf.com/content/CVPR2023W/MULA/papers/Xue_Dynamic_Multimodal_Fusion_CVPRW_2023_paper.pdf"
            },
            {
                id: 14, title: "LLaVA-Mini: Efficient Multimodal Models with One Vision Token", venue: "arXiv", year: 2025, authors: "Shaolei Zhang et al.",
                summary: "Reduces vision tokens from 576 to 1 using modality pre-fusion. Achieves 77% FLOPs reduction and 40ms latency. Supports images and videos.",
                accuracy: 84.0, flops: 0.8, params: 130, tags: ["Token Compression", "SOTA"], link: "https://arxiv.org/abs/2501.03895"
            },
            {
                id: 15, title: "InternVL3.5: Visual Resolution Router for Efficiency", venue: "arXiv", year: 2025, authors: "Wenhai Wang et al.",
                summary: "Visual Resolution Router (ViR) dynamically adjusts visual token resolution. Decoupled deployment separates vision/language across GPUs. Achieves 4.05x inference speedup.",
                accuracy: 88.0, flops: 2.0, params: 400, tags: ["Dynamic Resolution", "SOTA"], link: "https://arxiv.org/abs/2508.18265"
            },
            {
                id: 16, title: "MoMa: Mixture of Modality-Aware Experts", venue: "arXiv", year: 2024, authors: "Xi Victoria Lin et al.",
                summary: "Modality-aware MoE architecture for efficient early-fusion pre-training. Achieves 3.7x overall FLOPs savings vs dense baseline through modality-specific routing.",
                accuracy: 82.5, flops: 1.0, params: 200, tags: ["MoE", "Pre-training"], link: "https://arxiv.org/pdf/2407.21770"
            },
            {
                id: 17, title: "QLIP: Text-Aligned Visual Tokenization", venue: "arXiv", year: 2025, authors: "Yue Zhao et al.",
                summary: "Quantized Language-Image Pretraining combining reconstruction quality with zero-shot understanding. Drop-in replacement for LLaVA visual encoder.",
                accuracy: 85.0, flops: 1.3, params: 90, tags: ["Tokenization", "Quantized"], link: "https://arxiv.org/pdf/2502.05178"
            },
            {
                id: 18, title: "OneEncoder: Lightweight Multimodal Framework", venue: "Neural Comp.", year: 2025, authors: "Bilal Faye et al.",
                summary: "Two-step training: Universal Projection for image-text, then frozen with alignment layer for new modalities. Reduces training costs and paired data needs.",
                accuracy: 80.0, flops: 1.1, params: 35, tags: ["Lightweight", "Progressive"], link: "https://arxiv.org/pdf/2409.11059"
            },
            {
                id: 19, title: "MQuant: Static Quantization for MLLMs", venue: "ACM International Conference on Multimedia", year: 2025, authors: "Jiayi Yu et al.",
                summary: "Post-training W4A8 quantization for MLLMs. Addresses visual token latency and outliers. Achieves <1% accuracy degradation with 30% latency reduction.",
                accuracy: 86.0, flops: 0.9, params: 180, tags: ["Quantization", "Inference"], link: "https://dl.acm.org/doi/pdf/10.1145/3746027.3755433"
            },
            {
                id: 20, title: "mPLUG-Owl3: Long Image-Sequence Understanding", venue: "arXiv", year: 2024, authors: "Jiabo Ye et al.",
                summary: "Hyper Attention Blocks (HABT) for efficient vision-language integration. Sparse replacement of Transformer blocks enables long multi-image scenarios.",
                accuracy: 84.5, flops: 1.7, params: 250, tags: ["Long-Context", "Sparse"], link: "https://arxiv.org/abs/2408.04840"
            },

            // ========== NEW 18 PAPERS ==========
            {
                id: 21, title: "Flamingo: Visual Language Model for Few-Shot Learning", venue: "NeurIPS", year: 2022, authors: "Jean-Baptiste Alayrac et al. (DeepMind)",
                summary: "80B parameter VLM using Perceiver Resampler and gated cross-attention. Bridges frozen vision encoders with frozen LLMs. Achieves SOTA few-shot learning on 16 benchmarks.",
                accuracy: 82.0, flops: 4.5, params: 80000, tags: ["Few-Shot", "Cross-Attention"], link: "https://arxiv.org/abs/2204.14198"
            },
            {
                id: 22, title: "MobileViT: Light-weight Vision Transformer", venue: "ICLR", year: 2022, authors: "Sachin Mehta, Mohammad Rastegari (Apple)",
                summary: "Hybrid CNN-Transformer for mobile. Combines local conv features with global transformer attention. 78.4% ImageNet with 2M params, 9x smaller than ResNet-50.",
                accuracy: 78.4, flops: 2.0, params: 2.0, tags: ["Hybrid", "Mobile"], link: "https://arxiv.org/abs/2110.02178"
            },
            {
                id: 23, title: "TinyViT: Fast Pretraining Distillation for Small ViTs", venue: "ECCV", year: 2022, authors: "Kan Wu et al. (Microsoft)",
                summary: "Memory-efficient knowledge distillation framework. Sparse teacher logits stored on disk. TinyViT-21M matches Swin-T accuracy with 2.5x fewer params.",
                accuracy: 84.8, flops: 4.3, params: 21, tags: ["Distillation", "Small ViT"], link: "https://arxiv.org/abs/2207.10666"
            },
            {
                id: 24, title: "FastV: Image Worth 1/2 Tokens After Layer 2", venue: "ECCV", year: 2024, authors: "Liang Chen et al.",
                summary: "Plug-and-play inference acceleration. Prunes 50% visual tokens after layer 2 based on attention scores. 45% FLOPs reduction without performance loss.",
                accuracy: 85.0, flops: 1.8, params: 13000, tags: ["Token Pruning", "Plug-and-Play"], link: "https://arxiv.org/abs/2403.06764"
            },
            {
                id: 25, title: "Token Merging (ToMe): Your ViT But Faster", venue: "ICLR", year: 2023, authors: "Daniel Bolya et al.",
                summary: "Training-free token merging using bipartite soft matching. Reduces tokens by 2x with minimal accuracy drop. Works with any ViT without modification.",
                accuracy: 82.5, flops: 1.5, params: 86, tags: ["Token Merging", "Training-Free"], link: "https://arxiv.org/abs/2210.09461"
            },
            {
                id: 26, title: "DynamicViT: Efficient ViT with Dynamic Token Sparsification", venue: "NeurIPS", year: 2021, authors: "Yongming Rao et al.",
                summary: "Progressively prunes uninformative tokens using lightweight prediction modules. Reduces FLOPs by 31-37% while maintaining accuracy. Uses KD for training.",
                accuracy: 81.3, flops: 2.9, params: 86, tags: ["Dynamic Pruning", "KD"], link: "https://arxiv.org/abs/2106.02034"
            },
            {
                id: 27, title: "FastVLM: Efficient Vision Encoding for VLMs", venue: "CVPR", year: 2025, authors: "Pavan Kumar Anasosalu Vasu et al. (Apple)",
                summary: "FastViTHD hybrid encoder achieves 85x faster TTFT than LLaVA-OneVision. 3.4x smaller vision encoder. Eliminates need for token pruning via architecture.",
                accuracy: 86.5, flops: 1.2, params: 35.7, tags: ["Hybrid Encoder", "Apple", "SOTA"], link: "https://arxiv.org/abs/2412.13303"
            },
            {
                id: 28, title: "Eve: Efficient VLMs with Elastic Visual Experts", venue: "AAAI", year: 2025, authors: "Rang et al.",
                summary: "1.8B parameter model with adaptable visual expertise. Balances linguistic and multimodal capabilities. Outperforms 7B LLaVA-1.5 in multimodal accuracy.",
                accuracy: 83.5, flops: 1.6, params: 1800, tags: ["Elastic", "Small Model"], link: "https://ojs.aaai.org/index.php/AAAI/article/view/32718"
            },
            {
                id: 29, title: "MiniCPM-V: Efficient MLLMs for Edge Devices", venue: "Nature Comm.", year: 2025, authors: "OpenBMB Team",
                summary: "8B model outperforms GPT-4V on 11 benchmarks. Runs on mobile phones. Supports 30+ languages, high-res images, robust OCR, low hallucination.",
                accuracy: 87.5, flops: 2.5, params: 8000, tags: ["Edge Deploy", "Multilingual"], link: "https://www.nature.com/articles/s41467-025-61040-5"
            },
            {
                id: 30, title: "SparseVLM: Visual Token Sparsification for Efficient VLM Inference", venue: "ICML", year: 2025, authors: "Yuan Zhang et al.",
                summary: "Training-free text-guided token pruning and merging. Uses attention from text tokens to rank image token importance per layer. Achieves 54% FLOPs reduction, 37% lower latency while retaining 97% accuracy.",
                accuracy: 84.0, flops: 1.4, params: 7000, tags: ["Sparse", "Text-Guided", "Training-Free"], link: "https://arxiv.org/abs/2410.04417"
            },
            {
                id: 31, title: "PVC: Progressive Visual Token Compression", venue: "CVPR", year: 2025, authors: "Yang et al.",
                summary: "Unified compression for images and videos. Progressive encoding with AdaLN layers. Reduces tokens per image from 256 to 64 while maintaining performance.",
                accuracy: 85.5, flops: 1.5, params: 8000, tags: ["Progressive", "Unified"], link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_PVC_Progressive_Visual_Token_Compression_for_Unified_Image_and_Video_CVPR_2025_paper.pdf"
            },
            {
                id: 32, title: "LongVU: Spatiotemporal Adaptive Compression", venue: "arXiv", year: 2024, authors: "Shen et al.",
                summary: "Dynamic frame sampling for long videos. Temporal token merging. Enables efficient processing of hour-long videos with minimal quality loss.",
                accuracy: 83.0, flops: 1.3, params: 7000, tags: ["Video", "Long-Context"], link: "https://arxiv.org/abs/2410.17434"
            },
            {
                id: 33, title: "CrossGET: Cross-Guided Ensemble of Tokens", venue: "ICML", year: 2024, authors: "Shi et al.",
                summary: "Cross-modal token guidance for accelerating vision-language transformers. Achieves 2x speedup through strategic token selection and ensemble.",
                accuracy: 82.0, flops: 1.8, params: 200, tags: ["Cross-Modal", "Ensemble"], link: "https://proceedings.mlr.press/v235/"
            },
            {
                id: 34, title: "PyramidDrop: Pyramid Visual Redundancy Reduction", venue: "arXiv", year: 2024, authors: "Xing et al.",
                summary: "Hierarchical token pruning with pyramid structure. Fewer tokens in early layers, more aggressive pruning later. Outperforms FastV on localization tasks.",
                accuracy: 84.5, flops: 1.4, params: 7000, tags: ["Pyramid", "Hierarchical"], link: "https://arxiv.org/abs/2410.17247"
            },
            {
                id: 35, title: "MoE-LLaVA: Mixture of Experts for Large VLMs", venue: "arXiv", year: 2024, authors: "Lin et al.",
                summary: "Sparse MoE architecture for VLMs. Only 3B active params from 7B total. Achieves comparable performance to dense 7B models with 40% less compute.",
                accuracy: 83.5, flops: 1.6, params: 3000, tags: ["MoE", "Sparse"], link: "https://arxiv.org/abs/2401.15947"
            },
            {
                id: 36, title: "LLaVA-PruMerge: Adaptive Token Reduction for LMMs", venue: "ICCV", year: 2025, authors: "Yuzhang Shang et al.",
                summary: "Dynamically determines token count per input. Uses [CLS]-token attention to identify important tokens, then clusters and merges similar ones. Reduces to ~40 tokens/image with ~10x lower FLOPs.",
                accuracy: 83.0, flops: 1.2, params: 7000, tags: ["Prune+Merge", "Adaptive", "CLS-guided"], link: "https://arxiv.org/abs/2403.15388"
            },
            {
                id: 37, title: "VisionZip: Longer is Better but Not Necessary", venue: "arXiv", year: 2024, authors: "Yang et al.",
                summary: "Training-free visual token compression. Identifies key tokens using attention patterns. 90%+ token reduction possible for simple tasks.",
                accuracy: 82.5, flops: 1.0, params: 7000, tags: ["Training-Free", "Compression"], link: "https://arxiv.org/abs/2412.04467"
            },
            {
                id: 38, title: "Kimi-VL: MoE Vision-Language Reasoning Model", venue: "arXiv", year: 2025, authors: "Moonshot AI",
                summary: "16B total params, 2.8B active. Long chain-of-thought fine-tuned with RL alignment. Uses MoonViT (SigLIP-so-400M) as vision encoder.",
                accuracy: 86.0, flops: 1.8, params: 2800, tags: ["MoE", "Reasoning", "RL"], link: "https://arxiv.org/abs/2504.07491"
            },
            {
                id: 39, title: "LIMoE: Multimodal Contrastive Learning with Sparse MoE", venue: "NeurIPS", year: 2022, authors: "Basil Mustafa et al.",
                summary: "First large-scale multimodal Transformer using sparse Mixture-of-Experts. Activates only subset of expert sub-networks per input. Achieves 84.1% zero-shot ImageNet accuracy at significantly lower per-token compute.",
                accuracy: 84.1, flops: 2.0, params: 5000, tags: ["MoE", "Sparse", "Contrastive"], link: "https://arxiv.org/abs/2206.02770"
            },
            {
                id: 41, title: "DyCoke: Dynamic Compression of Tokens for Fast Video LLMs", venue: "CVPR", year: 2025, authors: "Keda Tao et al.",
                summary: "Two-stage token compression for video-LLMs. Merges redundant tokens across frames (temporal), then prunes from KV cache (spatial). Retains ~15 tokens/frame, 1.5x speedup without fine-tuning.",
                accuracy: 84.0, flops: 1.0, params: 7000, tags: ["Video", "KV-Cache", "Temporal"], link: "http://openaccess.thecvf.com/content/CVPR2025/papers/Tao_DyCoke_Dynamic_Compression_of_Tokens_for_Fast_Video_Large_Language_CVPR_2025_paper.pdf"
            },
            {
                id: 42, title: "Token Sequence Compression for Efficient Multimodal Computing", venue: "arXiv", year: 2025, authors: "Yasmine Omri et al.",
                summary: "Extensive study of visual token selection and merging strategies. Found cluster-based token aggregation outperforms prior SOTA. Demonstrates significant redundancy in vision encoders.",
                accuracy: 83.5, flops: 1.1, params: 7000, tags: ["Survey", "Clustering", "Aggregation"], link: "https://arxiv.org/pdf/2504.17892"
            },
            {
                id: 43, title: "TopV: Compatible Token Pruning with Inference Time Optimization", venue: "CVPR", year: 2025, authors: "Yang et al.",
                summary: "Formulates token pruning as optimization during prefilling. Uses Sinkhorn algorithm with feature similarity, spatial distance, and central distance factors.",
                accuracy: 84.5, flops: 1.0, params: 7000, tags: ["Optimization", "Sinkhorn"], link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_TopV_Compatible_Token_Pruning_with_Inference_Time_Optimization_for_Fast_CVPR_2025_paper.pdf"
            },
            {
                id: 44, title: "PruneVid: Visual Token Pruning for Efficient Video Large Language Models", venue: "ACL", year: 2025, authors: "Various",
                summary: "Identifies static video regions with minimal temporal variation. Reduces redundancy via temporal token merging, then spatial clustering, followed by question-guided attention.",
                accuracy: 83.0, flops: 0.9, params: 7000, tags: ["Video", "Temporal", "Question-Guided"], link: "https://aclanthology.org/2025.findings-acl.1024.pdf"
            },
            {
                id: 45, title: "Dynamic-LLaVA: Efficient MLLM via Dynamic Vision-Language Context Sparsification", venue: "ICLR", year: 2025, authors: "Various",
                summary: "Dynamically sparsifies vision-language context based on input complexity. Adapts computation to content difficulty for efficient multimodal processing.",
                accuracy: 84.0, flops: 1.2, params: 7000, tags: ["Dynamic", "Context Sparsification"], link: "https://proceedings.iclr.cc/paper_files/paper/2025/file/aeafb73dfed3007ec5299be1604d6f99-Paper-Conference.pdf"
            },
            {
                id: 46, title: "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models", venue: "CVPR", year: 2025, authors: "Saeed Ranjbar Alvar, Gursimran Singh†, Mohammad Akbari†, Yong Zhang",
                summary: "Prunes visual tokens while preserving diversity. Ensures remaining tokens cover diverse visual concepts rather than just high-attention regions.",
                accuracy: 83.5, flops: 1.1, params: 7000, tags: ["Diversity", "Pruning"], link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Alvar_DivPrune_Diversity-based_Visual_Token_Pruning_for_Large_Multimodal_Models_CVPR_2025_paper.pdf"
            },
            {
                id: 47, title: "PACT: Pruning and Clustering Token Reduction for Faster VLMs", venue: "CVPR", year: 2025, authors: "Various",
                summary: "Two-stage approach: first prunes unimportant tokens, then clusters similar remaining tokens. Preserves semantic information while reducing compute.",
                accuracy: 84.0, flops: 1.0, params: 7000, tags: ["Pruning", "Clustering", "Two-Stage"], link: "https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_PACT_Pruning_and_Clustering_Token_Reduction_for_Faster_VLMs_CVPR_2025_paper.pdf"
            },
            {
                id: 48, title: "AVG-LLaVA: Large Multimodal Model with Adaptive Visual Granularity", venue: "ACL", year: 2025, authors: "Zhibin Lan, Liqiang Niu, Fandong Meng, Wenbo Li, Jie Zhou, Jinsong Su",
                summary: "Adapts visual token granularity based on image complexity. Simple images use fewer tokens, complex images retain more detail.",
                accuracy: 83.0, flops: 1.3, params: 7000, tags: ["Adaptive", "Granularity"], link: "https://aclanthology.org/2025.findings-acl.865.pdf"
            },
            {
                id: 49, title: "Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs", venue: "ACL", year: 2024, authors: "Various",
                summary: "Simple yet effective token reduction method. Demonstrates that straightforward approaches can match complex methods with proper implementation.",
                accuracy: 82.5, flops: 1.0, params: 7000, tags: ["Simple", "Effective"], link: "https://aclanthology.org/2025.coling-main.508.pdf"
            },
            {
                id: 50, title: "Don't Look Twice: Faster Video Transformers with Run-Length Tokenization", venue: "NeurIPS", year: 2024, authors: "Rohan Choudhury, Guanglei Zhu Sihan Liu, Koichiro Niinuma,Kris M. Kitani, László A. Jeni ",
                summary: "Don't Look Twice - exploits temporal redundancy in video via run-length encoding of tokens. Significantly reduces computation for video understanding.",
                accuracy: 82.0, flops: 0.8, params: 7000, tags: ["Video", "Run-Length", "Temporal"], link: "https://arxiv.org/pdf/2411.05222"
            },
            {
                id: 51, title: "HyperVL: Efficient Dynamic MLLM for Edge Devices", venue: "arXiv", year: 2024, authors: "Various",
                summary: "Image-tiling strategy with Visual Resolution Compressor (VRC) for adaptive encoding. Dual Consistency Learning aligns multi-scale ViT encoders.",
                accuracy: 83.0, flops: 1.2, params: 4000, tags: ["Edge", "Dynamic", "Multi-Scale"], link: "https://arxiv.org/abs/2512.14052"
            },
            {
                id: 52, title: "Qwen2-VL: Enhancing VLM Perception at Any Resolution", venue: "arXiv", year: 2024, authors: "Alibaba",
                summary: "Naive Dynamic Resolution mechanism processes images of varying resolutions into different visual token counts. More efficient and accurate visual representations.",
                accuracy: 86.5, flops: 2.0, params: 7000, tags: ["Dynamic Resolution", "Any Aspect"], link: "https://arxiv.org/abs/2409.12191"
            },
            {
                id: 53, title: "InternVL 2.5: Pixel Unshuffle for Efficient High-Resolution", venue: "arXiv", year: 2024, authors: "OpenGVLab",
                summary: "Pixel unshuffle reduces visual tokens from 1024 to 256 per 448x448 tile. Improves scalability for high-resolution image processing.",
                accuracy: 87.0, flops: 1.8, params: 8000, tags: ["Pixel Unshuffle", "High-Resolution"], link: "https://arxiv.org/pdf/2412.05271v1"
            },
            {
                id: 54, title: "OmniVLM: Sub-Billion Parameter VLM for On-Device Inference", venue: "arXiv", year: 2024, authors: "Various",
                summary: "Token compression reduces visual sequence from 729 to 81 tokens. Sub-billion parameters enable efficient on-device deployment while maintaining visual-semantic fidelity.",
                accuracy: 79.0, flops: 0.5, params: 800, tags: ["Sub-Billion", "On-Device", "Compression"], link: "https://arxiv.org/pdf/2412.11475"
            },
            {
                id: 55, title: "Window Token Concatenation for Efficient Visual Large Language Models", venue: "CVPR-W", year: 2025, authors: "Yifan Li, Wentao Bao, Botao Ye, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong",
                summary: "Concatenates tokens within spatial windows to reduce sequence length. Simple approach that maintains spatial relationships while improving efficiency.",
                accuracy: 82.5, flops: 1.1, params: 7000, tags: ["Window", "Concatenation"], link: "https://openaccess.thecvf.com/content/CVPR2025W/eLVM/papers/Li_Window_Token_Concatenation_for_Efficient_Visual_Large_Language_Models_CVPRW_2025_paper.pdf"
            },
            {
                id: 56, title: "HICom: Hierarchical Compression for Vision-Language Models", venue: "arxiv", year: 2025, authors: "Various",
                summary: "Multi-level hierarchical compression strategy. Compresses at patch, region, and image levels for maximum efficiency with minimal information loss.",
                accuracy: 84.0, flops: 0.9, params: 7000, tags: ["Hierarchical", "Multi-Level"], link: "https://arxiv.org/pdf/2406.11884"
            },
            {
                id: 57, title: "TwigVLM: Efficient Visual Token Pruning via Tree Structure", venue: "ICCV", year: 2025, authors: "Various",
                summary: "Organizes visual tokens in tree structure for efficient pruning. Prunes entire branches of less important tokens while preserving key visual information.",
                accuracy: 83.5, flops: 1.0, params: 7000, tags: ["Tree Structure", "Branch Pruning"], link: "https://openaccess.thecvf.com/content/ICCV2025/papers/Shao_Growing_a_Twig_to_Accelerate_Large_Vision-Language_Models_ICCV_2025_paper.pdf"
            },
            {
                id: 58, title: "Efficient Large Multi-modal Models via Visual Context Compression", venue: "arXiv", year: 2024, authors: "Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille",
                summary: "Introduces Visual Context Compressor using average pooling and LLaVolta, a staged training scheme that progressively compresses visual tokens from heavy to light compression during training. Eliminating up to 70% of visual tokens leads to only 3% accuracy reduction, indicating significant redundancy. Achieves 16% training cost reduction and 24% inference latency improvement.",
                accuracy: 83.0, flops: 1.1, params: 7000, tags: ["Stage-wise", "Curriculum", "Visual Context Compressor"], link: "https://arxiv.org/abs/2406.20092v2"
            },
            {
                id: 59, title: "QG-VTC: Query-Guided Visual Token Compression", venue: "arXiv", year: 2024, authors: "Various",
                summary: "Computes token relevance by embedding user's question and correlating with vision tokens. Task-aware compression keeps only query-relevant visual information.",
                accuracy: 83.5, flops: 1.0, params: 7000, tags: ["Query-Guided", "Task-Aware"], link: "https://arxiv.org/pdf/2504.00654"
            },
            {
                id: 60, title: "Inference Optimal VLMs: Fewer Visual Tokens, More Parameters", venue: "ICLR", year: 2025, authors: "Various",
                summary: "Studies optimal trade-off between LLM size and visual tokens. Finds that fewer visual tokens with larger LLMs often outperforms more tokens with smaller LLMs.",
                accuracy: 85.0, flops: 1.5, params: 13000, tags: ["Scaling Laws", "Trade-off"], link: "https://arxiv.org/abs/2411.03312"
            },
            {
                id: 61, title: "LLaVolta: Stage-wise Visual Token Compression", venue: "arXiv", year: 2024, authors: "Various",
                summary: "Stage-wise training with heavy compression in early epochs/layers, gradually relaxed. Balances efficiency and performance through curriculum learning.",
                accuracy: 83.0, flops: 1.1, params: 7000, tags: ["Stage-wise", "Curriculum"], link: "https://arxiv.org/pdf/2406.20092v1"
            }
        ];

        // --- METHODOLOGY CATEGORIES ---
        const methodologyCategories = {
            'frozen-foundations': {
                title: 'Frozen Foundations',
                description: 'These methods leverage pre-trained Large Language Models (LLMs) without updating their weights. They introduce lightweight bridging modules such as Q-Former or Perceiver Resampler to align visual features with the frozen LLM\'s embedding space. This approach drastically reduces training costs while maintaining strong performance.',
                icon: 'fa-snowflake',
                tags: ['Frozen LLM', 'Q-Former', 'Cross-Attention', 'Few-Shot', 'Distillation', 'Low-Rank', 'Bottleneck', 'Linear Complexity', 'Fine-Tuning', 'Contrastive', 'Pre-training']
            },
            'dynamic-computation': {
                title: 'Dynamic Computation',
                description: 'These approaches utilize Reinforcement Learning (RL) or attention-based methods to dynamically adjust the visual token budget based on input complexity. Unlike static architectural pruning, these methods make runtime decisions about where to allocate compute, enabling adaptive efficiency.',
                icon: 'fa-bolt',
                tags: ['Dynamic', 'RL', 'MoE', 'Adaptive', 'Dynamic Resolution', 'Context Sparsification', 'Elastic', 'Gating', 'Dynamic Pruning', 'Reasoning', 'Scaling Laws', 'Trade-off', 'Any Aspect', 'Granularity', 'Multi-Scale']
            },
            'hybrid-architectures': {
                title: 'Hybrid Architectures',
                description: 'These architectures combine the local feature extraction capabilities of Convolutional Neural Networks (CNNs) with the global attention mechanisms of Transformers. This hybrid approach achieves mobile-friendly efficiency without sacrificing accuracy, making them ideal for edge deployment.',
                icon: 'fa-layer-group',
                tags: ['Hybrid', 'Mobile', 'Edge', 'Edge Deploy', 'Hybrid Architectures', 'Quantization', 'Small ViT', 'Sub-Billion', 'On-Device', 'Lightweight', 'Hybrid Encoder', 'Apple', 'Small Model', 'Low-Cost', 'Attention', 'Inference', 'Open-Source', 'Benchmark', 'Multilingual', 'High-Resolution', 'Pixel Unshuffle', 'Quantized', 'Tokenization']
            },
            'token-compression': {
                title: 'Token Compression',
                description: 'Token compression methods reduce visual tokens via merging, pruning, or compression techniques. These methods can achieve 50-77% FLOPs reduction while maintaining task performance. Many are training-free and can be applied to existing models as plug-and-play solutions.',
                icon: 'fa-compress-alt',
                tags: ['Token Compression', 'Token Pruning', 'Token Merging', 'Sparse', 'Compression', 'Pruning', 'Clustering', 'Prune+Merge', 'Training-Free', 'Hierarchical', 'Progressive', 'Text-Guided', 'Unified', 'Cross-Modal', 'Ensemble', 'Pyramid', 'KV-Cache', 'Temporal', 'Survey', 'Aggregation', 'Optimization', 'Sinkhorn', 'Question-Guided', 'Diversity', 'Two-Stage', 'Simple', 'Effective', 'Run-Length', 'Window', 'Concatenation', 'Multi-Level', 'Tree Structure', 'Branch Pruning', 'Stage-wise', 'Curriculum', 'Visual Context Compressor', 'Task-Aware', 'KD', 'Plug-and-Play', 'CLS-guided', 'SOTA', 'Video', 'Long-Context']
            }
        };

        // --- PAPER EXTENDED DATA ---
        // Store methodology details and future directions for each paper
        const paperExtendedData = {};

        // Generate default extended data for papers
        papers.forEach(p => {
            paperExtendedData[p.id] = {
                methodology_detail: generateMethodologyDetail(p),
                future_directions: generateFutureDirections(p),
                annotated_pdf: localStorage.getItem(`paper_pdf_${p.id}`) || null
            };
        });

        function generateMethodologyDetail(paper) {
            const tagBasedDetails = {
                'Frozen LLM': 'This approach keeps the Large Language Model (LLM) weights frozen during training, only optimizing a lightweight bridging module to align visual features with the LLM\'s embedding space.',
                'Q-Former': 'Utilizes a Querying Transformer (Q-Former) that learns a set of query embeddings to extract visual features relevant to the language model through cross-attention mechanisms.',
                'Token Pruning': 'Implements selective removal of less informative visual tokens based on attention scores or learned importance weights, reducing computational overhead during inference.',
                'Token Merging': 'Combines similar tokens using bipartite matching or clustering algorithms, preserving semantic information while reducing the total number of tokens processed.',
                'RL': 'Employs Reinforcement Learning to train a policy that decides which visual regions or tokens to process, optimizing for both accuracy and computational efficiency.',
                'MoE': 'Uses Mixture of Experts architecture where only a subset of expert networks are activated per input, achieving sparse computation while maintaining model capacity.',
                'Dynamic': 'Adapts computation at runtime based on input complexity, allocating more resources to difficult samples and less to simpler ones.',
                'Hybrid': 'Combines convolutional layers for local feature extraction with transformer layers for global context modeling, leveraging the strengths of both architectures.',
                'Quantization': 'Reduces model precision from floating-point to lower-bit representations, decreasing memory footprint and enabling faster inference on specialized hardware.',
                'Distillation': 'Transfers knowledge from a larger teacher model to a smaller student model, achieving competitive performance with significantly fewer parameters.'
            };

            let details = paper.summary + '\n\n';
            paper.tags.forEach(tag => {
                if (tagBasedDetails[tag]) {
                    details += tagBasedDetails[tag] + ' ';
                }
            });
            return details.trim() || paper.summary;
        }

        function generateFutureDirections(paper) {
            const directions = [];

            if (paper.tags.some(t => ['Token Pruning', 'Token Merging', 'Token Compression'].includes(t))) {
                directions.push('• Exploring adaptive token budgets that vary based on task difficulty and input complexity.');
                directions.push('• Investigating the combination of pruning and merging strategies for maximum efficiency.');
            }
            if (paper.tags.some(t => ['RL', 'Dynamic'].includes(t))) {
                directions.push('• Extending dynamic computation to multi-modal reasoning chains.');
                directions.push('• Developing more efficient policy networks for token selection.');
            }
            if (paper.tags.some(t => ['MoE'].includes(t))) {
                directions.push('• Investigating expert specialization patterns for different visual domains.');
                directions.push('• Optimizing load balancing across experts for improved training stability.');
            }
            if (paper.tags.some(t => ['Hybrid', 'Mobile', 'Edge'].includes(t))) {
                directions.push('• Hardware-aware architecture search for specific edge devices.');
                directions.push('• Exploring neural architecture search for optimal CNN-Transformer combinations.');
            }
            if (paper.tags.some(t => ['Video', 'Long-Context'].includes(t))) {
                directions.push('• Scaling to longer video sequences with memory-efficient attention.');
                directions.push('• Temporal modeling improvements for better video understanding.');
            }

            // Default directions
            if (directions.length === 0) {
                directions.push('• Scaling the approach to larger model sizes and more diverse datasets.');
                directions.push('• Investigating cross-task transfer learning capabilities.');
                directions.push('• Exploring combinations with other efficiency techniques.');
            }

            directions.push('• Benchmarking on emerging multimodal tasks and real-world applications.');

            return directions.join('\n');
        }

        // --- ROUTER ---
        let currentView = 'main';
        let currentPaperId = null;
        let currentMethodology = null;

        function navigateTo(route) {
            window.location.hash = route ? `/${route}` : '/';
        }

        function handleRoute() {
            const hash = window.location.hash.slice(1) || '/';
            const parts = hash.split('/').filter(p => p);

            // Hide all views
            document.querySelectorAll('.view').forEach(v => v.classList.remove('active'));

            if (parts.length === 0 || parts[0] === '') {
                // Main view
                showMainView();
            } else if (parts[0] === 'methodology' && parts[1]) {
                // Methodology view
                showMethodologyView(parts[1]);
            } else if (parts[0] === 'paper' && parts[1]) {
                // Paper detail view
                showPaperView(parseInt(parts[1]));
            } else {
                // Default to main
                showMainView();
            }

            // Scroll to top
            window.scrollTo(0, 0);
        }

        function showMainView() {
            currentView = 'main';
            document.getElementById('view-main').classList.add('active');
        }

        function showMethodologyView(methodologySlug) {
            currentView = 'methodology';
            currentMethodology = methodologySlug;
            const category = methodologyCategories[methodologySlug];

            if (!category) {
                navigateTo('');
                return;
            }

            // Update breadcrumb
            document.getElementById('methodology-breadcrumb').textContent = category.title;

            // Update header
            document.getElementById('methodology-title').textContent = category.title;
            document.getElementById('methodology-description').textContent = category.description;
            document.getElementById('methodology-icon').innerHTML = `<i class="fas ${category.icon}"></i>`;

            // Filter papers by tags
            const categoryPapers = papers.filter(p =>
                p.tags.some(tag => category.tags.includes(tag))
            );

            // Update stats
            document.getElementById('methodology-count').textContent = categoryPapers.length;
            const avgAcc = categoryPapers.length > 0
                ? (categoryPapers.reduce((sum, p) => sum + p.accuracy, 0) / categoryPapers.length).toFixed(1)
                : 0;
            const avgFlops = categoryPapers.length > 0
                ? (categoryPapers.reduce((sum, p) => sum + p.flops, 0) / categoryPapers.length).toFixed(1)
                : 0;
            document.getElementById('methodology-avg-acc').textContent = avgAcc + '%';
            document.getElementById('methodology-avg-flops').textContent = avgFlops + 'G';

            // Render papers
            renderMethodologyPapers(categoryPapers, methodologySlug);

            // Show view
            document.getElementById('view-methodology').classList.add('active');
        }

        function renderMethodologyPapers(paperList, methodologySlug) {
            const grid = document.getElementById('methodology-paper-grid');
            grid.innerHTML = '';

            if (paperList.length === 0) {
                grid.innerHTML = `<div class="col-span-full py-10 text-center text-slate-400 italic">No papers found in this category.</div>`;
                return;
            }

            paperList.forEach(p => {
                const card = document.createElement('div');
                card.className = "group bg-white dark:bg-slate-900 border border-slate-200 dark:border-slate-800 rounded-lg p-5 hover:border-academic-400 dark:hover:border-academic-600 transition-colors cursor-pointer flex flex-col h-full";
                card.onclick = () => navigateTo(`paper/${p.id}`);

                card.innerHTML = `
                    <div class="flex justify-between items-start mb-3">
                        <span class="inline-flex items-center px-2 py-1 rounded text-[10px] font-bold uppercase tracking-wider bg-slate-100 text-slate-600 dark:bg-slate-800 dark:text-slate-400">
                            ${p.venue} ${p.year}
                        </span>
                        ${p.year === 2025 ? '<span class="w-2 h-2 rounded-full bg-academic-500"></span>' : ''}
                    </div>
                    <h3 class="font-bold text-slate-900 dark:text-white mb-1 leading-snug group-hover:text-academic-600 dark:group-hover:text-academic-400 transition-colors">${p.title}</h3>
                    <p class="text-xs text-slate-500 dark:text-slate-400 italic mb-4">${p.authors}</p>
                    <p class="text-sm text-slate-600 dark:text-slate-400 line-clamp-2 mb-4 flex-grow font-serif">${p.summary}</p>
                    <div class="flex items-center justify-between mt-auto pt-3 border-t border-slate-100 dark:border-slate-800">
                        <div class="flex gap-4 text-xs">
                            <span class="text-slate-500"><strong class="text-slate-700 dark:text-slate-300">${p.accuracy}%</strong> Acc</span>
                            <span class="text-slate-500"><strong class="text-slate-700 dark:text-slate-300">${p.flops}G</strong> FLOPs</span>
                        </div>
                        <i class="fas fa-arrow-right text-academic-500 opacity-0 group-hover:opacity-100 transition-opacity"></i>
                    </div>
                `;
                grid.appendChild(card);
            });
        }

        function showPaperView(paperId) {
            currentView = 'paper';
            currentPaperId = paperId;
            const paper = papers.find(p => p.id === paperId);

            if (!paper) {
                navigateTo('');
                return;
            }

            const extendedData = paperExtendedData[paperId] || {};

            // Find which methodology this paper belongs to
            let paperMethodology = null;
            for (const [slug, cat] of Object.entries(methodologyCategories)) {
                if (paper.tags.some(tag => cat.tags.includes(tag))) {
                    paperMethodology = { slug, ...cat };
                    break;
                }
            }

            // Update breadcrumb
            const methodologyLink = document.getElementById('paper-methodology-link');
            if (paperMethodology) {
                methodologyLink.textContent = paperMethodology.title;
                methodologyLink.onclick = (e) => { e.preventDefault(); navigateTo(`methodology/${paperMethodology.slug}`); };
            } else {
                methodologyLink.textContent = 'Papers';
                methodologyLink.onclick = (e) => { e.preventDefault(); navigateTo(''); };
            }
            document.getElementById('paper-breadcrumb').textContent = paper.title;

            // Update header
            document.getElementById('paper-venue-badge').textContent = `${paper.venue} ${paper.year}`;
            document.getElementById('paper-year-dot').classList.toggle('hidden', paper.year !== 2025);
            document.getElementById('paper-title').textContent = paper.title;
            document.getElementById('paper-authors').textContent = paper.authors;

            // Update metrics
            document.getElementById('paper-accuracy').textContent = paper.accuracy + '%';
            document.getElementById('paper-flops').textContent = paper.flops + 'G';
            document.getElementById('paper-params').textContent = paper.params + 'M';

            // Update content sections
            document.getElementById('paper-summary').textContent = paper.summary;
            document.getElementById('paper-methodology-detail').textContent = extendedData.methodology_detail || paper.summary;
            document.getElementById('paper-future-directions').innerHTML = (extendedData.future_directions || '').replace(/\n/g, '<br>');

            // Update tags
            const tagsContainer = document.getElementById('paper-tags');
            tagsContainer.innerHTML = paper.tags.map(t =>
                `<span class="text-sm font-mono text-academic-600 dark:text-academic-400 border border-academic-200 dark:border-academic-800 px-3 py-1 rounded-full">${t}</span>`
            ).join('');

            // Update source link
            document.getElementById('paper-source-link').href = paper.link;

            // Load and render markdown content
            loadPaperMarkdown(paperId);

            // Show view
            document.getElementById('view-paper').classList.add('active');
        }

        // Simple markdown to HTML converter
        function markdownToHTML(markdown) {
            let html = markdown;
            
            // Split into lines for processing
            const lines = html.split('\n');
            const processed = [];
            let inList = false;
            let listItems = [];
            
            for (let i = 0; i < lines.length; i++) {
                const line = lines[i].trim();
                
                // Headers
                if (line.startsWith('### ')) {
                    if (inList) {
                        processed.push('</ul>');
                        inList = false;
                    }
                    processed.push(`<h3 class="text-lg font-bold text-slate-900 dark:text-white mt-6 mb-3 sans-font">${line.substring(4)}</h3>`);
                    continue;
                }
                if (line.startsWith('## ')) {
                    if (inList) {
                        processed.push('</ul>');
                        inList = false;
                    }
                    processed.push(`<h2 class="text-xl font-bold text-slate-900 dark:text-white mt-8 mb-4 sans-font">${line.substring(3)}</h2>`);
                    continue;
                }
                if (line.startsWith('# ')) {
                    if (inList) {
                        processed.push('</ul>');
                        inList = false;
                    }
                    processed.push(`<h1 class="text-2xl font-bold text-slate-900 dark:text-white mt-8 mb-4 sans-font">${line.substring(2)}</h1>`);
                    continue;
                }
                
                // Horizontal rules
                if (line === '---') {
                    if (inList) {
                        processed.push('</ul>');
                        inList = false;
                    }
                    processed.push('<hr class="my-6 border-slate-200 dark:border-slate-700">');
                    continue;
                }
                
                // List items
                if (line.match(/^[-•]\s+/)) {
                    if (!inList) {
                        inList = true;
                        listItems = [];
                    }
                    const itemText = line.replace(/^[-•]\s+/, '');
                    listItems.push(`<li class="ml-4 mb-1 text-slate-700 dark:text-slate-300">${processInlineMarkdown(itemText)}</li>`);
                    continue;
                }
                
                // End of list - close when encountering empty line OR non-list content
                if (inList) {
                    if (line === '') {
                        // Empty line closes the list
                        processed.push(`<ul class="list-disc list-inside space-y-1 mb-4">${listItems.join('')}</ul>`);
                        inList = false;
                        listItems = [];
                        continue;
                    } else if (!line.match(/^[-•]\s+/)) {
                        // Non-list content closes the list first, then process the content
                        processed.push(`<ul class="list-disc list-inside space-y-1 mb-4">${listItems.join('')}</ul>`);
                        inList = false;
                        listItems = [];
                        // Continue processing this line as regular content (don't continue here)
                    }
                }
                
                // Regular paragraphs (only if not in list, or list was just closed above)
                if (line && !inList) {
                    processed.push(`<p class="mb-4 text-slate-700 dark:text-slate-300 leading-relaxed">${processInlineMarkdown(line)}</p>`);
                } else if (!line && !inList) {
                    // Empty line
                    continue;
                }
            }
            
            // Close any open list
            if (inList) {
                processed.push(`<ul class="list-disc list-inside space-y-1 mb-4">${listItems.join('')}</ul>`);
            }
            
            return processed.join('\n');
        }
        
        function processInlineMarkdown(text) {
            // Bold
            text = text.replace(/\*\*(.*?)\*\*/g, '<strong class="font-bold text-slate-900 dark:text-white">$1</strong>');
            
            // Italic
            text = text.replace(/\*([^*]+)\*/g, '<em class="italic">$1</em>');
            
            // Links
            text = text.replace(/\[([^\]]+)\]\(([^)]+)\)/g, '<a href="$2" target="_blank" class="text-academic-600 dark:text-academic-400 hover:underline">$1</a>');
            
            // Code (inline)
            text = text.replace(/`([^`]+)`/g, '<code class="bg-slate-200 dark:bg-slate-800 px-1.5 py-0.5 rounded text-sm font-mono text-slate-900 dark:text-slate-100">$1</code>');
            
            return text;
        }

        async function loadPaperMarkdown(paperId) {
            const loadingEl = document.getElementById('paper-markdown-loading');
            const contentEl = document.getElementById('paper-markdown-content');
            const errorEl = document.getElementById('paper-markdown-error');
            
            // Show loading state
            loadingEl.classList.remove('hidden');
            contentEl.classList.add('hidden');
            errorEl.classList.add('hidden');
            
            try {
                const paper = papers.find(p => p.id === paperId);
                if (!paper) {
                    throw new Error('Paper not found');
                }
                
                // Create filename: pad ID with zeros and sanitize title
                const paddedId = String(paperId).padStart(2, '0');
                // Sanitize title to match Python script logic exactly
                // Python order: 1) Remove special chars, 2) Replace spaces/hyphens, 3) Lowercase, 4) Truncate, 5) Strip hyphens
                let sanitizedTitle = paper.title
                    .replace(/[^\w\s-]/g, '')  // Step 1: Remove special chars (keep word chars, spaces, hyphens)
                    .replace(/[-\s]+/g, '-')   // Step 2: Replace spaces and multiple hyphens with single hyphen
                    .toLowerCase();             // Step 3: Convert to lowercase
                
                // Step 4: Limit length to 100 characters (BEFORE stripping hyphens)
                if (sanitizedTitle.length > 100) {
                    sanitizedTitle = sanitizedTitle.substring(0, 100);
                }
                
                // Step 5: Remove leading/trailing hyphens (AFTER truncation)
                sanitizedTitle = sanitizedTitle.replace(/^-+|-+$/g, '');
                
                const markdownPath = `blogpost/papers/${paddedId}-${sanitizedTitle}.md`;
                
                // Fetch markdown file
                const response = await fetch(markdownPath);
                if (!response.ok) {
                    // Try alternative: just by ID pattern (in case title doesn't match exactly)
                    const altPath = `blogpost/papers/${paddedId}-*.md`;
                    // Since we can't glob, try a few common variations
                    const variations = [
                        markdownPath,
                        `blogpost/papers/${paddedId}-${paper.title.toLowerCase().replace(/[^a-z0-9]/g, '-').substring(0, 50)}.md`
                    ];
                    
                    let loaded = false;
                    for (const path of variations) {
                        try {
                            const altResponse = await fetch(path);
                            if (altResponse.ok) {
                                const markdown = await altResponse.text();
                                const html = markdownToHTML(markdown);
                                contentEl.innerHTML = html;
                                loadingEl.classList.add('hidden');
                                contentEl.classList.remove('hidden');
                                loaded = true;
                                break;
                            }
                        } catch (e) {
                            continue;
                        }
                    }
                    
                    if (!loaded) {
                        throw new Error(`Failed to load markdown file`);
                    }
                    return;
                }
                
                const markdown = await response.text();
                
                // Convert markdown to HTML
                const html = markdownToHTML(markdown);
                
                // Display content
                contentEl.innerHTML = html;
                loadingEl.classList.add('hidden');
                contentEl.classList.remove('hidden');
                
            } catch (error) {
                console.error('Error loading markdown:', error);
                loadingEl.classList.add('hidden');
                errorEl.classList.remove('hidden');
            }
        }

        // --- GITHUB OAUTH ---
        // IMPORTANT: Replace with your Cloudflare Worker URL after deployment
        // See workers/github-oauth-proxy.js for setup instructions
        const OAUTH_WORKER_URL = localStorage.getItem('oauth_worker_url') || 'https://github-oauth-proxy.YOUR_SUBDOMAIN.workers.dev';

        let githubAccessToken = localStorage.getItem('github_access_token') || null;
        let githubClientId = localStorage.getItem('github_client_id') || '';
        let deviceCodeInterval = null;
        let currentDeviceCode = null;

        function openGitHubModal() {
            document.getElementById('github-oauth-modal').classList.remove('hidden');
            document.getElementById('github-client-id').value = githubClientId;
            document.getElementById('github-worker-url').value = localStorage.getItem('oauth_worker_url') || '';
            showOAuthStep(1);
        }

        function closeGitHubModal() {
            document.getElementById('github-oauth-modal').classList.add('hidden');
            cancelGitHubAuth();
        }

        function showOAuthStep(step) {
            document.querySelectorAll('.oauth-step').forEach(s => s.classList.add('hidden'));
            document.getElementById(`oauth-step-${step}`).classList.remove('hidden');
            document.getElementById('oauth-error').classList.add('hidden');
        }

        function showOAuthError(message) {
            document.getElementById('oauth-error').classList.remove('hidden');
            document.getElementById('oauth-error-text').textContent = message;
        }

        async function startGitHubAuth() {
            const workerUrl = document.getElementById('github-worker-url').value.trim();
            const clientId = document.getElementById('github-client-id').value.trim();

            if (!workerUrl) {
                showOAuthError('Please enter your Cloudflare Worker URL.');
                return;
            }
            if (!clientId) {
                showOAuthError('Please enter your GitHub OAuth App Client ID.');
                return;
            }

            // Save to localStorage
            localStorage.setItem('oauth_worker_url', workerUrl);
            githubClientId = clientId;
            localStorage.setItem('github_client_id', clientId);

            try {
                // Request device code via Cloudflare Worker
                const response = await fetch(`${workerUrl}/device/code`, {
                    method: 'POST',
                    headers: {
                        'Accept': 'application/json',
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        client_id: clientId,
                        scope: 'repo'
                    })
                });

                if (!response.ok) {
                    throw new Error('Failed to get device code. Check your Worker URL and Client ID.');
                }

                const data = await response.json();

                if (data.error) {
                    throw new Error(data.error_description || data.error);
                }
                currentDeviceCode = data.device_code;

                // Show user code
                document.getElementById('github-user-code').textContent = data.user_code;
                showOAuthStep(2);

                // Open GitHub device page
                window.open(data.verification_uri, '_blank');

                // Start polling for token
                const interval = data.interval || 5;
                deviceCodeInterval = setInterval(() => pollForToken(clientId, data.device_code), interval * 1000);

            } catch (error) {
                showOAuthError(error.message);
            }
        }

        async function pollForToken(clientId, deviceCode) {
            try {
                // Poll for access token via Cloudflare Worker
                const workerUrl = localStorage.getItem('oauth_worker_url');
                const response = await fetch(`${workerUrl}/access_token`, {
                    method: 'POST',
                    headers: {
                        'Accept': 'application/json',
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        client_id: clientId,
                        device_code: deviceCode
                    })
                });

                const data = await response.json();

                if (data.access_token) {
                    // Success!
                    githubAccessToken = data.access_token;
                    localStorage.setItem('github_access_token', data.access_token);
                    clearInterval(deviceCodeInterval);
                    showOAuthStep(3);
                    showToast('Successfully authenticated with GitHub!', 'success');
                } else if (data.error === 'authorization_pending') {
                    // Still waiting, continue polling
                } else if (data.error === 'slow_down') {
                    // Slow down polling
                } else if (data.error === 'expired_token') {
                    clearInterval(deviceCodeInterval);
                    showOAuthError('Authorization expired. Please try again.');
                    showOAuthStep(1);
                } else if (data.error === 'access_denied') {
                    clearInterval(deviceCodeInterval);
                    showOAuthError('Authorization was denied.');
                    showOAuthStep(1);
                }
            } catch (error) {
                // Network error, keep polling
            }
        }

        function cancelGitHubAuth() {
            if (deviceCodeInterval) {
                clearInterval(deviceCodeInterval);
                deviceCodeInterval = null;
            }
            currentDeviceCode = null;
        }

        function isGitHubAuthenticated() {
            return !!githubAccessToken;
        }

        // --- TOAST NOTIFICATIONS ---
        function showToast(message, type = 'success') {
            const toast = document.getElementById('toast');
            const icon = document.getElementById('toast-icon');
            const messageEl = document.getElementById('toast-message');

            messageEl.textContent = message;
            icon.className = type === 'success'
                ? 'fas fa-check-circle text-green-400 dark:text-green-600'
                : 'fas fa-exclamation-circle text-red-400 dark:text-red-600';

            toast.classList.remove('hidden');

            setTimeout(() => {
                toast.classList.add('hidden');
            }, 3000);
        }

        // --- THEME LOGIC ---
        function toggleTheme() {
            const html = document.documentElement;
            const isDark = html.classList.toggle('dark');
            html.classList.toggle('light', !isDark);
            localStorage.setItem('theme', isDark ? 'dark' : 'light');
            updateChartTheme(isDark);
        }

        function loadTheme() {
            const saved = localStorage.getItem('theme') || 'light';
            document.documentElement.classList.add(saved);
            if(saved === 'dark') document.documentElement.classList.remove('light');
        }

        // --- CHART LOGIC ---
        let chartInstance = null;
        let currentMode = 'flops';

        function initChart() {
            const ctx = document.getElementById('frontierChart').getContext('2d');
            const isDark = document.documentElement.classList.contains('dark');

            // Academic Color Palette (Extended for more venues)
            const getBg = (v) => {
                if (v === 'ICLR') return 'rgba(125, 211, 252, 0.7)'; // Light Blue
                if (v === 'ICML') return 'rgba(14, 165, 233, 0.7)';  // Blue
                if (v === 'CVPR') return 'rgba(34, 197, 94, 0.7)';   // Green
                if (v === 'ECCV') return 'rgba(249, 115, 22, 0.7)';  // Orange
                if (v === 'ICCV') return 'rgba(234, 179, 8, 0.7)';   // Yellow
                if (v === 'AAAI') return 'rgba(168, 85, 247, 0.7)';  // Purple
                if (v === 'Nature Comm.') return 'rgba(236, 72, 153, 0.7)'; // Pink
                return 'rgba(2, 132, 199, 0.7)'; // Default Blue (NeurIPS)
            };
            const getBorder = (v) => {
                if (v === 'ICLR') return '#7dd3fc';
                if (v === 'ICML') return '#0ea5e9';
                if (v === 'CVPR') return '#22c55e';
                if (v === 'ECCV') return '#f97316';
                if (v === 'ICCV') return '#eab308';
                if (v === 'AAAI') return '#a855f7';
                if (v === 'Nature Comm.') return '#ec4899';
                return '#0284c7';
            };

            const dataPoints = papers.map(p => ({
                x: p.flops, y: p.accuracy, r: p.year === 2025 ? 12 : (p.year === 2024 ? 9 : 7), paper: p
            }));

            chartInstance = new Chart(ctx, {
                type: 'bubble',
                data: {
                    datasets: [{
                        label: 'Papers',
                        data: dataPoints,
                        backgroundColor: (c) => getBg(c.raw?.paper?.venue),
                        borderColor: (c) => getBorder(c.raw?.paper?.venue),
                        borderWidth: 1.5
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    plugins: {
                        legend: { display: false },
                        tooltip: {
                            backgroundColor: isDark ? '#0f172a' : '#ffffff',
                            titleColor: isDark ? '#f8fafc' : '#0f172a',
                            bodyColor: isDark ? '#cbd5e1' : '#334155',
                            borderColor: isDark ? '#1e293b' : '#e2e8f0',
                            borderWidth: 1,
                            padding: 10,
                            titleFont: { family: 'Inter', weight: 'bold' },
                            bodyFont: { family: 'Inter' },
                            callbacks: {
                                label: (ctx) => ctx.raw.paper.title,
                                afterLabel: (ctx) => ` Venue: ${ctx.raw.paper.venue} '${ctx.raw.paper.year} | Acc: ${ctx.raw.y}%`
                            }
                        }
                    },
                    scales: {
                        x: {
                            title: { display: true, text: 'Computational Cost (GFLOPs)', font: { family: 'Inter' } },
                            grid: { color: isDark ? '#1e293b' : '#f1f5f9' }
                        },
                        y: {
                            title: { display: true, text: 'Accuracy (%)', font: { family: 'Inter' } },
                            grid: { color: isDark ? '#1e293b' : '#f1f5f9' },
                            min: 50
                        }
                    }
                }
            });
            updateChartTheme(isDark);
        }

        function updateChartTheme(isDark) {
            if (!chartInstance) return;
            const textColor = isDark ? '#94a3b8' : '#64748b';
            const gridColor = isDark ? '#1e293b' : '#e2e8f0';

            chartInstance.options.scales.x.title.color = textColor;
            chartInstance.options.scales.x.ticks.color = textColor;
            chartInstance.options.scales.x.grid.color = gridColor;
            chartInstance.options.scales.y.title.color = textColor;
            chartInstance.options.scales.y.ticks.color = textColor;
            chartInstance.options.scales.y.grid.color = gridColor;

            chartInstance.options.plugins.tooltip.backgroundColor = isDark ? '#0f172a' : '#ffffff';
            chartInstance.options.plugins.tooltip.titleColor = isDark ? '#f8fafc' : '#0f172a';
            chartInstance.options.plugins.tooltip.bodyColor = isDark ? '#cbd5e1' : '#334155';
            chartInstance.options.plugins.tooltip.borderColor = isDark ? '#1e293b' : '#e2e8f0';

            chartInstance.update();
        }

        function updateChartMode(mode) {
            currentMode = mode;
            const btnFlops = document.getElementById('btn-flops');
            const btnParams = document.getElementById('btn-params');

            if(mode === 'flops') {
                btnFlops.className = "px-4 py-2 text-xs font-medium text-white bg-academic-600 border border-academic-600 rounded-l-lg hover:bg-academic-700 transition-all";
                btnParams.className = "px-4 py-2 text-xs font-medium text-slate-700 bg-white border border-slate-200 rounded-r-lg hover:bg-slate-50 dark:bg-slate-800 dark:border-slate-700 dark:text-white dark:hover:bg-slate-700 transition-all";
                chartInstance.options.scales.x.title.text = 'Computational Cost (GFLOPs)';
                chartInstance.data.datasets[0].data = papers.map(p => ({ x: p.flops, y: p.accuracy, r: p.year === 2025 ? 12 : (p.year === 2024 ? 9 : 7), paper: p }));
            } else {
                btnParams.className = "px-4 py-2 text-xs font-medium text-white bg-academic-600 border border-academic-600 rounded-r-lg hover:bg-academic-700 transition-all";
                btnFlops.className = "px-4 py-2 text-xs font-medium text-slate-700 bg-white border border-slate-200 rounded-l-lg hover:bg-slate-50 dark:bg-slate-800 dark:border-slate-700 dark:text-white dark:hover:bg-slate-700 transition-all";
                chartInstance.options.scales.x.title.text = 'Model Size (Millions of Params)';
                chartInstance.data.datasets[0].data = papers.map(p => ({ x: p.params, y: p.accuracy, r: p.year === 2025 ? 12 : (p.year === 2024 ? 9 : 7), paper: p }));
            }
            chartInstance.update();
        }

        // --- GRID RENDERING ---
        function renderPapers(list) {
            const grid = document.getElementById('paper-grid');
            grid.innerHTML = '';

            if (list.length === 0) {
                grid.innerHTML = `<div class="col-span-full py-10 text-center text-slate-400 italic">No matches found within the literature.</div>`;
                return;
            }

            list.forEach(p => {
                const card = document.createElement('div');
                card.className = "group bg-white dark:bg-slate-900 border border-slate-200 dark:border-slate-800 rounded-lg p-5 hover:border-academic-400 dark:hover:border-academic-600 transition-colors cursor-pointer flex flex-col h-full";
                card.onclick = () => openModal(p);

                const isNew = p.id > 20;

                card.innerHTML = `
                    <div class="flex justify-between items-start mb-3">
                        <span class="inline-flex items-center px-2 py-1 rounded text-[10px] font-bold uppercase tracking-wider bg-slate-100 text-slate-600 dark:bg-slate-800 dark:text-slate-400">
                            ${p.venue} ${p.year}
                        </span>
                        <div class="flex gap-1">
                            ${isNew ? '<span class="px-1.5 py-0.5 rounded text-[9px] font-bold bg-green-100 text-green-700 dark:bg-green-900 dark:text-green-300">NEW</span>' : ''}
                            ${p.year === 2025 ? '<span class="w-2 h-2 rounded-full bg-academic-500"></span>' : ''}
                        </div>
                    </div>
                    <h3 class="font-bold text-slate-900 dark:text-white mb-1 leading-snug group-hover:text-academic-600 dark:group-hover:text-academic-400 transition-colors">${p.title}</h3>
                    <p class="text-xs text-slate-500 dark:text-slate-400 italic mb-4">${p.authors}</p>
                    <p class="text-sm text-slate-600 dark:text-slate-400 line-clamp-2 mb-4 flex-grow font-serif">${p.summary}</p>
                    <div class="flex flex-wrap gap-2 mt-auto">
                        ${p.tags.map(t => `<span class="text-[10px] font-mono text-academic-600 dark:text-academic-400 border border-academic-100 dark:border-academic-900 px-1.5 py-0.5 rounded">${t}</span>`).join('')}
                    </div>
                `;
                grid.appendChild(card);
            });
        }

        function filterPapers() {
            const search = document.getElementById('search-input').value.toLowerCase();
            const venue = document.getElementById('venue-filter').value;

            const filtered = papers.filter(p => {
                const matchesSearch = p.title.toLowerCase().includes(search) ||
                                      p.authors.toLowerCase().includes(search) ||
                                      p.tags.some(t => t.toLowerCase().includes(search));
                const matchesVenue = venue === 'all' || p.venue === venue;
                return matchesSearch && matchesVenue;
            });
            renderPapers(filtered);
        }

        // --- MODAL ---
        const modal = document.getElementById('paper-modal');
        let modalPaperId = null;

        function openModal(paper) {
            modalPaperId = paper.id;
            document.getElementById('modal-title').innerText = paper.title;
            document.getElementById('modal-authors').innerText = paper.authors;
            document.getElementById('modal-summary').innerText = paper.summary;
            document.getElementById('modal-link').href = paper.link;
            document.getElementById('modal-venue').innerText = `${paper.venue} ${paper.year}`;
            document.getElementById('modal-acc').innerText = paper.accuracy + '%';
            document.getElementById('modal-flops').innerText = paper.flops + 'G';
            document.getElementById('modal-params').innerText = paper.params + 'M';

            // Set up View Details button
            document.getElementById('modal-details-btn').onclick = () => {
                closeModal();
                navigateTo(`paper/${paper.id}`);
            };

            modal.classList.remove('hidden');
        }
        function closeModal() {
            modal.classList.add('hidden');
        }

        // --- INIT ---
        document.addEventListener('DOMContentLoaded', () => {
            loadTheme();
            initChart();
            renderPapers(papers);

            // Initialize router
            window.addEventListener('hashchange', handleRoute);
            handleRoute(); // Handle initial route
        });
    </script>
</body>
</html>
