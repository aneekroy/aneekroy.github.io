# TinyViT: Fast Pretraining Distillation for Small ViTs

**Authors:** Kan Wu et al. (Microsoft)  
**Venue:** ECCV 2022  
**Paper ID:** 23

## Abstract

Memory-efficient knowledge distillation framework. Sparse teacher logits stored on disk. TinyViT-21M matches Swin-T accuracy with 2.5x fewer params.

## Metrics

- **Accuracy:** 84.8%
- **FLOPs:** 4.3G
- **Parameters:** 21M

## Tags

`Distillation`, `Small ViT`

## Methodology

Memory-efficient knowledge distillation framework. Sparse teacher logits stored on disk. TinyViT-21M matches Swin-T accuracy with 2.5x fewer params.

Transfers knowledge from a larger teacher model to a smaller student model, achieving competitive performance with significantly fewer parameters.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2207.10666](https://arxiv.org/abs/2207.10666)

---

 
