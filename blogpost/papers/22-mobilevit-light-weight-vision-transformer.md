# MobileViT: Light-weight Vision Transformer

**Authors:** Sachin Mehta, Mohammad Rastegari (Apple)  
**Venue:** ICLR 2022  
**Paper ID:** 22

## Abstract

Hybrid CNN-Transformer for mobile. Combines local conv features with global transformer attention. 78.4% ImageNet with 2M params, 9x smaller than ResNet-50.

## Metrics

- **Accuracy:** 78.4%
- **FLOPs:** 2.0G
- **Parameters:** 2.0M

## Tags

`Hybrid`, `Mobile`

## Methodology

Hybrid CNN-Transformer for mobile. Combines local conv features with global transformer attention. 78.4% ImageNet with 2M params, 9x smaller than ResNet-50.

Combines convolutional layers for local feature extraction with transformer layers for global context modeling, leveraging the strengths of both architectures.

## Future Directions

• Hardware-aware architecture search for specific edge devices.
• Exploring neural architecture search for optimal CNN-Transformer combinations.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2110.02178](https://arxiv.org/abs/2110.02178)

---

 
