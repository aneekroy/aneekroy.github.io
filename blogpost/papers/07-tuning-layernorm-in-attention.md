# Tuning LayerNorm in Attention

**Authors:** Bingchen Zhao et al.  
**Venue:** ICLR 2024  
**Paper ID:** 7

## Abstract

Showed that only tuning LayerNorm parameters in attention blocks yields robust adaptation, outperforming LoRA with 42% fewer trainable params.

## Metrics

- **Accuracy:** 82.0%
- **FLOPs:** 2.8G
- **Parameters:** 0.5M

## Tags

`Fine-Tuning`

## Methodology

Showed that only tuning LayerNorm parameters in attention blocks yields robust adaptation, outperforming LoRA with 42% fewer trainable params.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2312.11420](https://arxiv.org/abs/2312.11420)

---

*Generated from blogpost.html survey data*
