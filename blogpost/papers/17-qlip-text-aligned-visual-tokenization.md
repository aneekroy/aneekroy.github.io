# QLIP: Text-Aligned Visual Tokenization

**Authors:** Yue Zhao et al.  
**Venue:** arXiv 2025  
**Paper ID:** 17

## Abstract

Quantized Language-Image Pretraining combining reconstruction quality with zero-shot understanding. Drop-in replacement for LLaVA visual encoder.

## Metrics

- **Accuracy:** 85.0%
- **FLOPs:** 1.3G
- **Parameters:** 90M

## Tags

`Tokenization`, `Quantized`

## Methodology

Quantized Language-Image Pretraining combining reconstruction quality with zero-shot understanding. Drop-in replacement for LLaVA visual encoder.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/pdf/2502.05178](https://arxiv.org/pdf/2502.05178)

---

 
