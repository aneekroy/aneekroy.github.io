# Inference Optimal VLMs: Fewer Visual Tokens, More Parameters

**Authors:** Various  
**Venue:** ICLR 2025  
**Paper ID:** 60

## Abstract

Studies optimal trade-off between LLM size and visual tokens. Finds that fewer visual tokens with larger LLMs often outperforms more tokens with smaller LLMs.

## Metrics

- **Accuracy:** 85.0%
- **FLOPs:** 1.5G
- **Parameters:** 13.0B

## Tags

`Scaling Laws`, `Trade-off`

## Methodology

Studies optimal trade-off between LLM size and visual tokens. Finds that fewer visual tokens with larger LLMs often outperforms more tokens with smaller LLMs.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2411.03312](https://arxiv.org/abs/2411.03312)

---

*Generated from blogpost.html survey data*
