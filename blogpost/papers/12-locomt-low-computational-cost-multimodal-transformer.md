# LoCoMT: Low-Computational-Cost Multimodal Transformer

**Authors:** Sungeun Park, Edward Choi  
**Venue:** ICASSP 2024  

## Abstract

Novel multimodal attention mechanism to reduce computational cost. Mitigates quadratic complexity as modalities increase. Reduces GFLOPs while matching performance.

## Metrics

- **Accuracy:** 81.0%
- **FLOPs:** 1.4G
- **Parameters:** 85M

## Tags

`Low-Cost`, `Attention`

## Methodology

Novel multimodal attention mechanism to reduce computational cost. Mitigates quadratic complexity as modalities increase. Reduces GFLOPs while matching performance.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://ieeexplore.ieee.org](https://ieeexplore.ieee.org)

---

 
