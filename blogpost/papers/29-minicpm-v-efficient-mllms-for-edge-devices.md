# MiniCPM-V: Efficient MLLMs for Edge Devices

**Authors:** OpenBMB Team  
**Venue:** Nature Comm. 2025  
**Paper ID:** 29

## Abstract

8B model outperforms GPT-4V on 11 benchmarks. Runs on mobile phones. Supports 30+ languages, high-res images, robust OCR, low hallucination.

## Metrics

- **Accuracy:** 87.5%
- **FLOPs:** 2.5G
- **Parameters:** 8.0B

## Tags

`Edge Deploy`, `Multilingual`

## Methodology

8B model outperforms GPT-4V on 11 benchmarks. Runs on mobile phones. Supports 30+ languages, high-res images, robust OCR, low hallucination.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://www.nature.com/articles/s41467-025-61040-5](https://www.nature.com/articles/s41467-025-61040-5)

---

*Generated from blogpost.html survey data*
