# LLaVA-Mini: Efficient Multimodal Models with One Vision Token

**Authors:** Shaolei Zhang et al.  
**Venue:** arXiv 2025  
**Paper ID:** 14

## Abstract

Reduces vision tokens from 576 to 1 using modality pre-fusion. Achieves 77% FLOPs reduction and 40ms latency. Supports images and videos.

## Metrics

- **Accuracy:** 84.0%
- **FLOPs:** 0.8G
- **Parameters:** 130M

## Tags

`Token Compression`, `SOTA`

## Methodology

Reduces vision tokens from 576 to 1 using modality pre-fusion. Achieves 77% FLOPs reduction and 40ms latency. Supports images and videos.

## Future Directions

• Exploring adaptive token budgets that vary based on task difficulty and input complexity.
• Investigating the combination of pruning and merging strategies for maximum efficiency.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2501.03895](https://arxiv.org/abs/2501.03895)

---

*Generated from blogpost.html survey data*
