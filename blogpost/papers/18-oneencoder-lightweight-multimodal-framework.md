# OneEncoder: Lightweight Multimodal Framework

**Authors:** Bilal Faye et al.  
**Venue:** Neural Comp. 2025  
**Paper ID:** 18

## Abstract

Two-step training: Universal Projection for image-text, then frozen with alignment layer for new modalities. Reduces training costs and paired data needs.

## Metrics

- **Accuracy:** 80.0%
- **FLOPs:** 1.1G
- **Parameters:** 35M

## Tags

`Lightweight`, `Progressive`

## Methodology

Two-step training: Universal Projection for image-text, then frozen with alignment layer for new modalities. Reduces training costs and paired data needs.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/pdf/2409.11059](https://arxiv.org/pdf/2409.11059)

---

 
