# DynamicViT: Efficient ViT with Dynamic Token Sparsification

**Authors:** Yongming Rao et al.  
**Venue:** NeurIPS 2021  
**Paper ID:** 26

## Abstract

Progressively prunes uninformative tokens using lightweight prediction modules. Reduces FLOPs by 31-37% while maintaining accuracy. Uses KD for training.

## Metrics

- **Accuracy:** 81.3%
- **FLOPs:** 2.9G
- **Parameters:** 86M

## Tags

`Dynamic Pruning`, `KD`

## Methodology

Progressively prunes uninformative tokens using lightweight prediction modules. Reduces FLOPs by 31-37% while maintaining accuracy. Uses KD for training.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2106.02034](https://arxiv.org/abs/2106.02034)

---

 
