# Dynamic-LLaVA: Efficient MLLM via Dynamic Vision-Language Context Sparsification

**Authors:** Various  
**Venue:** ICLR 2025  
**Paper ID:** 45

## Abstract

Dynamically sparsifies vision-language context based on input complexity. Adapts computation to content difficulty for efficient multimodal processing.

## Metrics

- **Accuracy:** 84.0%
- **FLOPs:** 1.2G
- **Parameters:** 7.0B

## Tags

`Dynamic`, `Context Sparsification`

## Methodology

Dynamically sparsifies vision-language context based on input complexity. Adapts computation to content difficulty for efficient multimodal processing.

Adapts computation at runtime based on input complexity, allocating more resources to difficult samples and less to simpler ones.

## Future Directions

• Extending dynamic computation to multi-modal reasoning chains.
• Developing more efficient policy networks for token selection.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://proceedings.iclr.cc/paper_files/paper/2025/file/aeafb73dfed3007ec5299be1604d6f99-Paper-Conference.pdf](https://proceedings.iclr.cc/paper_files/paper/2025/file/aeafb73dfed3007ec5299be1604d6f99-Paper-Conference.pdf)

---

*Generated from blogpost.html survey data*
