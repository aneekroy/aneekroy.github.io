# Efficient Large Multi-modal Models via Visual Context Compression

**Authors:** Jieneng Chen, Luoxin Ye, Ju He, Zhao-Yang Wang, Daniel Khashabi, Alan Yuille  
**Venue:** arXiv 2024  
**Paper ID:** 58

## Abstract

Introduces Visual Context Compressor using average pooling and LLaVolta, a staged training scheme that progressively compresses visual tokens from heavy to light compression during training. Eliminating up to 70% of visual tokens leads to only 3% accuracy reduction, indicating significant redundancy. Achieves 16% training cost reduction and 24% inference latency improvement.

## Metrics

- **Accuracy:** 83.0%
- **FLOPs:** 1.1G
- **Parameters:** 7.0B

## Tags

`Stage-wise`, `Curriculum`, `Visual Context Compressor`

## Methodology

Introduces Visual Context Compressor using average pooling and LLaVolta, a staged training scheme that progressively compresses visual tokens from heavy to light compression during training. Eliminating up to 70% of visual tokens leads to only 3% accuracy reduction, indicating significant redundancy. Achieves 16% training cost reduction and 24% inference latency improvement.

## Future Directions

• Scaling the approach to larger model sizes and more diverse datasets.
• Investigating cross-task transfer learning capabilities.
• Exploring combinations with other efficiency techniques.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/abs/2406.20092v2](https://arxiv.org/abs/2406.20092v2)

---

*Generated from blogpost.html survey data*
