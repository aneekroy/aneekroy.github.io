# Don't Look Twice: Faster Video Transformers with Run-Length Tokenization

**Authors:** Rohan Choudhury, Guanglei Zhu Sihan Liu, Koichiro Niinuma,Kris M. Kitani, László A. Jeni   
**Venue:** NeurIPS 2024  

## Abstract

Don't Look Twice - exploits temporal redundancy in video via run-length encoding of tokens. Significantly reduces computation for video understanding.

## Metrics

- **Accuracy:** 82.0%
- **FLOPs:** 0.8G
- **Parameters:** 7.0B

## Tags

`Video`, `Run-Length`, `Temporal`

## Methodology

Don't Look Twice - exploits temporal redundancy in video via run-length encoding of tokens. Significantly reduces computation for video understanding.

## Future Directions

• Scaling to longer video sequences with memory-efficient attention.
• Temporal modeling improvements for better video understanding.
• Benchmarking on emerging multimodal tasks and real-world applications.

## Links

- **Paper:** [https://arxiv.org/pdf/2411.05222](https://arxiv.org/pdf/2411.05222)

---

 
